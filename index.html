<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Shengjun Tang</title>
  <meta name="author" content="Shengjun Tang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
        <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ğŸŒ</text></svg>">
        <link href="css/bootstrap.min.css" rel="stylesheet" />
	<link href="css/font-awesome.min.css" rel="stylesheet" type="text/css" />
	<link href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css" />
	<link href="https://fonts.googleapis.com/css?family=Merriweather:400,300,300italic,400italic,700,700italic,900,900italic" rel="stylesheet" type="text/css" /><!-- Plugin CSS -->
	<link href="css/magnific-popup.css" rel="stylesheet" />
	</head>
<body id="page-top">
	<script>
			  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
			  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
			  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
			  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

			  ga('create', 'UA-59354848-1', 'auto');
			  ga('send', 'pageview');
	</script>

	<script language="javascript" type="text/javascript">
		function showHide(shID) {
		   if (document.getElementById(shID)) {
		      if (document.getElementById(shID+'-show').style.display != 'none') {
		         document.getElementById(shID+'-show').style.display = 'none';
		         document.getElementById(shID).style.display = 'block';
		      }
		      else {
		         document.getElementById(shID+'-show').style.display = 'inline';
		         document.getElementById(shID).style.display = 'none';
		      }
		   }
		}
	</script>
	<nav class="navbar navbar-default navbar-fixed-top" id="mainNav">
		<div class="container-fluid">
			<div class="navbar-header">
				<button class="navbar-toggle collapsed" data-target="#bs-example-navbar-collapse-1" data-toggle="collapse" type="button"><span class="sr-only">Toggle navigation</span> Menu</button><a class="navbar-brand page-scroll" href="#page-top" style="color: black;text-decoration: none;font-size:16pt">Homepage</a>
			</div>
			<div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
				<ul class="nav navbar-nav navbar-top">
					<li><a class="page-scroll" href="#about" style="color: black;text-decoration: none;">About</a></li>
					<li><a class="page-scroll" href="#news" style="color: black;text-decoration: none;">News</a></li>
					<li><a class="page-scroll" href="#publications" style="color: black;text-decoration: none;">Publications</a></li>
					<li><a class="page-scroll" href="#researchprojects" style="color: black;text-decoration: none;">Research Projects</a></li>
					<li><a class="page-scroll" href="#awards" style="color: black;text-decoration: none;">Awards</a></li>
					<li><a class="page-scroll" href="#academicservices" style="color: black;text-decoration: none;">Academic Service</a></li>
					<li><a class="page-scroll" href="#activites" style="color: black;text-decoration: none;">Academic Activites</a></li>
				</ul>
			</div>
		</div>
	</nav><br/>
<section id="about">
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Shengjun Tang (Tommy)</name>
              </p>
              <p style="text-align:justify; text-justify:inter-ideograph;"> <a href="https://geospatial.szu.edu.cn/info/1006/3518.htm">Dr. Shengjun Tang</a> is currently an assistant professor/associate researcher at <a href="https://geospatial.szu.edu.cn/kjry/jsxl/2.htm">Shenzhen University</a>. He received his bachelor's and doctoral degrees from <a href="http://liesmars.whu.edu.cn/"> Wuhan University</a>. From 2014 to 2017, he visited <a href="https://www.polyu.edu.hk/lsgi/"> the Department of Land Surveying and Geo-Informatics (LSGI) at The Hong Kong Polytechnic University</a> as a research assistant. After completing his doctoral degree in 2017, he worked as a postdoctoral researcher in the team of Professor Renzhong Guo at <a href="https://geospatial.szu.edu.cn/kjry/jsxl/2.htm">Shenzhen University</a>.
              </p>
              <p style="text-align:justify; text-justify:inter-ideograph;" >His research interests include Laser and visual simultaneous localization and mapping (SLAM), 3D reconstruction, Scene understanding with deep learning, Virtual Geography and modeling theory, and applications.<br/>  <a href="mailto:shengjuntang@szu.edu.cn">(Email:shengjuntang@szu.edu.cn)</a>
              </p>
              <p style="text-align:center">
                <a href="mailto:shengjuntang@szu.edu.cn">Email</a> &nbsp/&nbsp
                <a href="data/CV_ShengjunTang.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=q-fQQqUAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/ShengjunTang">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="Shengjun%20tang.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="shengjuntangcircle1.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
 </section>
<section id="news">
       <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p>
                <ul>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">10/2023: Congratulations, we have published a paper titled "Robust Calibration of Vehicle Solid-state Lidar-Camera Perception System Using Line-Weighted Correspondences in Natural Environments" in IEEE Transactions on Intelligent Transportation Systems</li>
	            <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">09/2023: Congratulations, our team won the Best Workshop Presentation Award at the ISPRS Geospatial Week 2023 International Conference for 'Tree-GPT: Modular Large Language Model Expert System for Forest Remote Sensing Image Understanding and Interactive Analysis'.</a></li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">09/2023: We organize a special issue in Remote Sensing. <a href="https://www.grss-ieee.org/wp-content/uploads/2023/07/cfp_High-fidelity-Urban-3D-Modeling-and-Scene-Simulation.pdf">Special issue on Remote Sensing for 2D/3D Mappingâ€</a>. Welcomes contributions from researchers and experts in the field. Submission Date End: 31 Mar 2024.</li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">08/2023: Congratulations, we have published a paper titled "Skeleton-guided generation of synthetic noisy point clouds from as-built BIM to improve indoor scene understanding" in the Automation in Construction journal. The publicly available 3D point cloud simulation dataset provided by the paper can be accessed at the following link:<a href="https://szueducn-my.sharepoint.com/:f:/g/personal/shengjuntang_szu_edu_cn/EgpOf3leEiFOjtfQTl-k7GgBougQGuQFcHDAbgtBZmVZ5w?e=rPPlot"> BIMSyn Dataset.</a></li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">07/2023: We organize a special issue in IEEE JSTAR.<a href="https://www.grss-ieee.org/wp-content/uploads/2023/07/cfp_High-fidelity-Urban-3D-Modeling-and-Scene-Simulation.pdf">Special issue on â€œHigh-fidelity Urban 3D Modeling and Scene Simulationâ€</a>. Welcomes contributions from researchers and experts in the field. Submission Date Start: 01 Oct 2023; Submission Date End: 30 Apr 2024.</li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">05/2023: One papers is submitted to ISPRS Journal</li>
	            <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">03/2023: One papers are submitted to IEEE T-ITS</li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">11/2022: One papers is submitted to Automation in Construction</li>
                </ul>
              </p>
            </td>
          </tr>
        </tbody>
        </table>
</section>
<section id="publications">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Publications</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/asymformer.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://browse.arxiv.org/pdf/2309.14065.pdf">
                <papertitle>AsymFormer: Asymmetrical Cross-Modal Representation Learning for Mobile Platform Real-Time RGB-D Semantic Segmentation (arXiv preprint)</papertitle>
              </a>
              <br>
              Siqi Du, Weixi Wang, Renzhong Guo and <strong>Shengjun Tang*</strong>
              <br>
              Submitted to ICRA 2024
              <br>
              <a href="https://browse.arxiv.org/pdf/2309.14065.pdf"> Link </a> |  <a href="https://github.com/Fourier7754/AsymFormer"> Code </a> 
              <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;">  In this work, we propose AsymFormer, a novel network for real-time RGB-D semantic segmentation, which targets the minimization of superfluous parameters by optimizing the distribution of computational resources and introduces an asymmetrical backbone to allow for the effective fusion of multimodal features..</a>
              </p>
            </td>
          </tr>
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/calib.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/EeEA--97IxxGjppxKUhz1j8B36Z6Gn8V8JXrEu4HUzf_Wg?e=nq5M6M">
                <papertitle>Robust Calibration of Vehicle Solid-state Lidar-Camera Perception System Using Line-Weighted Correspondences in Natural Environments(Accepted)</papertitle>
              </a>
              <br>
              <strong>Shengjun Tang</strong>, Yunqi Feng, Junjie Huang, Xiaoming Li, Zhihan Lv, Yuhong Feng, and Weixi Wang
              <br>
              IEEE Transactions on Intelligent Transportation Systems, 2023
              <br>
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/EblKSK0s8t9MrBvAmoHee9sBe29UyyoYpNNovXTqzIQRlg?e=wWnPAg"> Link </a> 
              <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;"> In this paper, we present a novel approach for robustly calibrating the extrinsic parameters of a solid-state(SS) lidar-camera system in a natural environment. Our proposed method begins with obtaining robust line feature information.</a>
              </p>
            </td>
          </tr>
        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/Syndatasets_AIC2023.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/EeEA--97IxxGjppxKUhz1j8B36Z6Gn8V8JXrEu4HUzf_Wg?e=nq5M6M">
                <papertitle>Skeleton-guided generation of synthetic noisy point clouds from as-built BIM to improve indoor scene understanding</papertitle>
              </a>
              <br>
              <strong>Shengjun Tang</strong>, Hongsheng Huang, Yunjie Zhang, Mengmeng Yao, Xiaoming Li, Linfu Xie, and Weixi Wang
              <br>
              Automation in Construction, 2023
              <br>
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/EeEA--97IxxGjppxKUhz1j8B36Z6Gn8V8JXrEu4HUzf_Wg?e=nq5M6M"> Link </a> | <a href="https://szueducn-my.sharepoint.com/:f:/g/personal/shengjuntang_szu_edu_cn/EmAdsrCIxrtEl21hocr8NMwB11DDjCJJM9ATA8Yzt1uNLQ?e=Ze0r5j"> Datasets </a> 
              <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;"> In this study, a fully automatic method to generate synthetic noisy point clouds from as-built building information modeling (BIM) models is presented and it assesses the potential of these synthetic point clouds to improve deep neural network training. All simulation datasets are publicly available, including original BIM models, full synthetic point clouds, and point clouds after IHPR processing, accessible via the following link: <a href="https://szueducn-my.sharepoint.com/:f:/g/personal/shengjuntang_szu_edu_cn/EgpOf3leEiFOjtfQTl-k7GgBougQGuQFcHDAbgtBZmVZ5w?e=rPPlot"> BIMSyn Dataset.</a>
              </p>
            </td>
          </tr>
        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/treemodeling.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="http://www.jors.cn/jrs/ch/reader/download_new_edit_content.aspx?file_no=202210300000001&flag=2">
                <papertitle>é¢å‘è™šæ‹Ÿåœ°ç†ç¯å¢ƒæ„å»ºçš„æ ‘æœ¨æ¨¡å‹é«˜ä¿çœŸä¸‰ç»´é‡å»ºæ–¹æ³•(A highly realistic 3D reconstruction method for tree models created for virtual geographic environments) </papertitle>
              </a>
              <br>
              ç‹ä¼Ÿçº(Weixi Wang), é»„é¸¿ç››(Hongsheng Huang),æœæ€é½(Siqi Du), ææ™“æ˜(Xiaoming Li), è°¢æ—ç”«(Linfu Xie), éƒ­ä»å¿ (Renzhong Guo), <strong>æ±¤åœ£å›(Shengjun Tang)*</strong>
              <br>
              é¥æ„Ÿå­¦æŠ¥(Journal of Remote Sensing), 2023
              <br>
              <a href="http://www.jors.cn/jrs/ch/reader/download_new_edit_content.aspx?file_no=202210300000001&flag=2"> Link </a> 
              <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;"> æœ¬æ–‡é¢å‘è™šæ‹Ÿåœ°ç†ç¯å¢ƒé«˜é€¼çœŸåœºæ™¯æ„å»ºéœ€æ±‚ï¼Œæå‡ºä¸€ç§åŸºäºé«˜ç²¾åº¦æ¿€å…‰æ‰«æç‚¹äº‘æ•°æ®çš„æ ‘æœ¨ä¸‰ç»´æ¨¡å‹é«˜ä¿çœŸä»¿ç”Ÿé‡å»ºæ–¹æ³•ï¼Œå¯å®ç°å½¢æ€ç‰¹å¾ä¿æŒçš„æ ‘æœ¨ä¸‰ç»´æ¨¡å‹è‡ªåŠ¨åŒ–é‡å»ºã€‚(In this paper, we propose a bionic reconstruction method for 3D tree models based on high-precision laser scanning point cloud data for building realistic scenes in virtual geographic environments, which enables the automated reconstruction of 3D tree models at multiple levels of detail while preserving morphological features.)     
              </p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/indoorclassification.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="http://ch.whu.edu.cn/article/exportPdf?id=dbb0171b-533f-4d8a-a3d7-67e694032f82">
                <papertitle>è¶…ä½“ç´ éšæœºæ£®æ—ä¸ LSTM ç¥ç»ç½‘ç»œè”åˆä¼˜åŒ–çš„å®¤å†…ç‚¹äº‘é«˜ç²¾åº¦åˆ†ç±»æ–¹æ³•(A High-Precision Indoor Point Cloud Classification Method Jointly Optimized by Super Voxel Random Forest and LSTM Neural Network) </papertitle>
              </a>
              <br>
              <strong>æ±¤åœ£å›(Shengjun Tang)</strong>, å¼ éŸµå©•(Yunjie Zhang), ææ™“æ˜(Xiaoming Li), å§šèŒèŒ(Mengmeng Yao), å¶è‡´ç…Œ(Zhihuang Ye), æäºšé‘«(Yaxin Li), éƒ­ä»å¿ (Renzhong Guo), ç‹ä¼Ÿçº(Weixi Wang)
              <br>
              æ­¦æ±‰å¤§å­¦å­¦æŠ¥ï¼ˆä¿¡æ¯ç§‘å­¦ç‰ˆï¼‰(Geomatics and Information Science of Wuhan University), 2023
              <br>
              <a href="http://ch.whu.edu.cn/article/exportPdf?id=dbb0171b-533f-4d8a-a3d7-67e694032f82">Link </a>
              <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;"> é’ˆå¯¹ç°æœ‰ä¸‰ç»´ç‚¹äº‘æ•°æ®åˆ†å‰²åˆ†ç±»æ–¹æ³•å­˜åœ¨åˆ†ç±»ç›®æ ‡å†…éƒ¨ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œæå‡ºä¸€ç§è¶…ä½“ç´ éšæœºæ£®æ—ä¸é•¿çŸ­æœŸè®°å¿†ç¥ç»ç½‘ç»œ(long short-term memoryï¼ŒLSTM)è”åˆä¼˜åŒ–çš„å®¤å†…ç‚¹äº‘é«˜ç²¾åº¦åˆ†ç±»æ–¹æ³•ã€‚  (In response to the problem of internal inconsistency in the classification targets of existing 3D point cloud data segmentation methods, we propose a high-precision indoor point cloud classification method that jointly optimizes super-voxel random forests and Long Short-Term Memory (LSTM) neural networks.)   
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/jag_pond.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://pdf.sciencedirectassets.com/272637/1-s2.0-S1569843223X0003X/1-s2.0-S1569843223001486/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEG0aCXVzLWVhc3QtMSJGMEQCIDb2spK%2F67VOb7fKXQu6A28R%2FRfteSf3Hf8U7wPXBjukAiATFZB7Xj9ZbJ8C0NSC%2FGp6SaZ%2F%2BZfomOFtn5MO0%2BCDHCq8BQiG%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAUaDDA1OTAwMzU0Njg2NSIMhWzSdAUIL%2BGVk1LjKpAFzpyCoIDb4WysFCxf5mFDpTYMqLaFPMpZ64P62%2BJeWN%2BLT1TpKzLSNRaNgjel0oV%2FrZfMcpRqeo6avPA6Ltcz7ifE1XfTNgG5irB2KXZhMLLqd1PRz%2FrM7U5hSG1Pyp88gL0r1HQccMJnFalpkQbqP7fNdzJ0ZsVpRJv0DP%2BslM9gM6fF5zkD%2FyDv4OfzDQoDESIqpJzlJY%2FarESW7WaL5N0KJ1MgRVck%2FEZ1o1jr4xlorWjeMumuc532mQqPk7mpTKxssy53aUYr0pigHtSlCLNkAZSdQnYqjb7OR%2BFrCkETGG1Lyvu0iW2ULdYOfGMlQezISpA8seINAE1MHvpdZQI%2BAyOFL5uVPWuTelJ7t7GE5ZW%2BO%2F40oHzGfUnM7yVdXZo60CEEXrtv1wFxHhJRIHsei%2FtVqE7RXCYNLJum8M3g85t%2FA4AkpkANmCzsH3HEDfaS5DMH2H7cxTEF4BEapdgvKVCoUkb2rg0A%2BZJvLNpkTUVob9JZRFcnKjZ1oDy5PLZwyS219xA7DxPRcROG1gOPg49SA7DiojyC8frOg6hhl%2FErOcRguHbgaVSC3AY5tyJHGnyLNviad0Fb4MEXlAVzCCwK03VpraBsYr5Oxv9ShZMnLE0OaaClGefDS%2F6ckxV91KYJRYSHs1GUQFQkFKXyoEhp6fjObHRd3U1zfAk3QSB%2FV%2B%2FDxaQb%2FodAVvU3O5aIg67byP3iPDcntxmUVsMsSYDNNLbqltyTEWbVwQCtIkS4te85Oo%2B1ieRuaSLWjA4Q%2F1Oulz3tA5naLvAu3dqLY07%2BI5Ub9csEsWBgM0E5DzfFyUQhip7r3CmQ7dYv4gFToGcknMVQKWtMwBkwkP%2FgoHJ61f6FrEp5maAz6Ccw2b3sogY6sgES391tlNmRCrSzKJVA90AqGJRHt5%2BlTMNeobV7JEtrKCkvwJDERzgT2PPCClAkmOn7dlVU%2BudScI9XCRhB%2FBNLXKeqELbrwtfH7jZMnnJeUExhKLOClOxVOQ7jRY5EARBOCmzuWuh5vg6Q1B4XQS8qZUOoMdtuOTIJ%2B8A%2ByUh0leH9AAabPmmBuqpEFqwevTFzxeT0aDICXucd%2BgcKKhDHZz4c%2B92Y8KAfDOcWCsGZHxyx&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20230510T055327Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY3JMUZDOO%2F20230510%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=8ee5e89b67d4c3354e3898e0bcfa81f8c52542ffbd2acc2a441762189f289148&hash=a7a8c4a72931447bc5128a10de27e8eeb9ec0e2a0d54ed10d3d07aabb3f38547&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S1569843223001486&tid=spdf-a022235e-841c-42db-a0d0-c7d388617534&sid=db751d415e6a084c0c3a0af-48fdc60f973dgxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=0c10500155040600545d&rr=7c4fec03db5225e7&cc=jp">
                <papertitle>Unsupervised stepwise extraction of offshore aquaculture ponds using super-resolution hyperspectral images </papertitle>
              </a>
              <br>
	      Siqi Du, Hongsheng Huang, Fan He, Heng Luo, Yumeng Yin, Xiaoming Li, Linfu Xie, Renzhong Guo,  <strong>Shengjun Tang*</strong>
              <br>
              International Journal of Applied Earth Observation and Geoinformation, 2023 (SCI, Top)
              <br>
              <a href="https://pdf.sciencedirectassets.com/272637/1-s2.0-S1569843223X0003X/1-s2.0-S1569843223001486/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEG0aCXVzLWVhc3QtMSJGMEQCIDb2spK%2F67VOb7fKXQu6A28R%2FRfteSf3Hf8U7wPXBjukAiATFZB7Xj9ZbJ8C0NSC%2FGp6SaZ%2F%2BZfomOFtn5MO0%2BCDHCq8BQiG%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAUaDDA1OTAwMzU0Njg2NSIMhWzSdAUIL%2BGVk1LjKpAFzpyCoIDb4WysFCxf5mFDpTYMqLaFPMpZ64P62%2BJeWN%2BLT1TpKzLSNRaNgjel0oV%2FrZfMcpRqeo6avPA6Ltcz7ifE1XfTNgG5irB2KXZhMLLqd1PRz%2FrM7U5hSG1Pyp88gL0r1HQccMJnFalpkQbqP7fNdzJ0ZsVpRJv0DP%2BslM9gM6fF5zkD%2FyDv4OfzDQoDESIqpJzlJY%2FarESW7WaL5N0KJ1MgRVck%2FEZ1o1jr4xlorWjeMumuc532mQqPk7mpTKxssy53aUYr0pigHtSlCLNkAZSdQnYqjb7OR%2BFrCkETGG1Lyvu0iW2ULdYOfGMlQezISpA8seINAE1MHvpdZQI%2BAyOFL5uVPWuTelJ7t7GE5ZW%2BO%2F40oHzGfUnM7yVdXZo60CEEXrtv1wFxHhJRIHsei%2FtVqE7RXCYNLJum8M3g85t%2FA4AkpkANmCzsH3HEDfaS5DMH2H7cxTEF4BEapdgvKVCoUkb2rg0A%2BZJvLNpkTUVob9JZRFcnKjZ1oDy5PLZwyS219xA7DxPRcROG1gOPg49SA7DiojyC8frOg6hhl%2FErOcRguHbgaVSC3AY5tyJHGnyLNviad0Fb4MEXlAVzCCwK03VpraBsYr5Oxv9ShZMnLE0OaaClGefDS%2F6ckxV91KYJRYSHs1GUQFQkFKXyoEhp6fjObHRd3U1zfAk3QSB%2FV%2B%2FDxaQb%2FodAVvU3O5aIg67byP3iPDcntxmUVsMsSYDNNLbqltyTEWbVwQCtIkS4te85Oo%2B1ieRuaSLWjA4Q%2F1Oulz3tA5naLvAu3dqLY07%2BI5Ub9csEsWBgM0E5DzfFyUQhip7r3CmQ7dYv4gFToGcknMVQKWtMwBkwkP%2FgoHJ61f6FrEp5maAz6Ccw2b3sogY6sgES391tlNmRCrSzKJVA90AqGJRHt5%2BlTMNeobV7JEtrKCkvwJDERzgT2PPCClAkmOn7dlVU%2BudScI9XCRhB%2FBNLXKeqELbrwtfH7jZMnnJeUExhKLOClOxVOQ7jRY5EARBOCmzuWuh5vg6Q1B4XQS8qZUOoMdtuOTIJ%2B8A%2ByUh0leH9AAabPmmBuqpEFqwevTFzxeT0aDICXucd%2BgcKKhDHZz4c%2B92Y8KAfDOcWCsGZHxyx&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20230510T055327Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY3JMUZDOO%2F20230510%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=8ee5e89b67d4c3354e3898e0bcfa81f8c52542ffbd2acc2a441762189f289148&hash=a7a8c4a72931447bc5128a10de27e8eeb9ec0e2a0d54ed10d3d07aabb3f38547&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S1569843223001486&tid=spdf-a022235e-841c-42db-a0d0-c7d388617534&sid=db751d415e6a084c0c3a0af-48fdc60f973dgxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=0c10500155040600545d&rr=7c4fec03db5225e7&cc=jp"> Link </a> 
              <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;"> In this paper, we proposed an unsupervised aquaculture ponds extraction method based on hyperspectral imagery super-resolution, feature fusion and stepwise extraction strategy. 
              </p>
            </td>
          </tr>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/treesegJSTAR.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10040735">
                <papertitle>An Individual Tree Segmentation Method from Mobile Mapping Point Clouds Based on Improved 3D Morphological Analysis</papertitle>
              </a>
              <br>
             Weixi Wang, Yuhang Fan, You Li, Xiaoming Li, <strong>Shengjun Tang*</strong>
              <br>
              IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2023
              <br>
              <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10040735"> Link </a> 
              <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;"> We propose a method based on improved 3D morphological analysis for extracting street trees from mobile laser scanner (MLS) point clouds. 
              </p>
            </td>
          </tr>


        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/AutoBIM_AIC2022.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/Ed6Z8EtiPuJCnmUxBtFyRh4BybZIVqXjb0ooq_NgcrynmA?e=k4vy8o">
                <papertitle>BIM generation from 3D point clouds by combining 3D deep learning and improved morphological approach</papertitle>
              </a>
              <br>
             <strong> Shengjun Tang</strong>, Xiaoming Li, Xianwei Zheng, Bo Wu, Weixi Wang, Yunjie Zhang
              <br>
              Automation In Construction, 2022 (SCI, Top)
              <br>
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/Ed6Z8EtiPuJCnmUxBtFyRh4BybZIVqXjb0ooq_NgcrynmA?e=k4vy8o"> Link </a> 
              <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;"> This paper presents a novel parametric modeling method for reconstructing semantic volumetric building interiors from the unstructured point cloud of a building. Unlike existing partitioning-based methods, our proposed method overcomes the limitations by providing a flexible framework for combining 3D Deep Learning and an improved morphological approach for inverse BIM modeling. 
              </p>
            </td>
          </tr>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/superV_RS2022.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.mdpi.com/2072-4292/14/6/1516/pdf">
                <papertitle>A Supervoxel-Based Random Forest Method for Robust and Effective Airborne LiDAR Point Cloud Classification</papertitle>
              </a>
              <br>
             Lingfeng Liao, <strong> Shengjun Tang*</strong>, Jianghai Liao, Xiaoming Li, Weixi Wang, Yaxin Li, Renzhong Guo
              <br>
              Remote Sensing, 2022 (SCI, Top)
              <br>
              <a href="https://www.mdpi.com/2072-4292/14/6/1516/pdf"> Link </a> 
              <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;"> In this paper, we propose a robust and effective point cloud classification approach that integrates point cloud supervoxels and their locally convex connected patches into a random forest classifier, which effectively improves the point cloud feature calculation accuracy and reduces the computational cost. 
              </p>
            </td>
          </tr>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/pscnet_isprsannals2022.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://isprs-annals.copernicus.org/articles/V-1-2022/129/2022/isprs-annals-V-1-2022-129-2022.pdf">
                <papertitle>PSCNET: EFFICIENT RGB-D SEMANTIC SEGMENTATION PARALLEL NETWORK BASED ON SPATIAL AND CHANNEL ATTENTION </papertitle>
              </a>
              <br>
              Siqi Du, <strong>Shengjun Tang*</strong>, Weixi Wang, Xiaoming Li, Yonghua Lu, Renzhong Guo
              <br>
              ISPRS Annals, 2022
              <br>
              <a href="https://isprs-annals.copernicus.org/articles/V-1-2022/129/2022/isprs-annals-V-1-2022-129-2022.pdf"> Link </a> 
              <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;"> RGB-D semantic segmentation algorithm is a key technology for indoor semantic map construction. The traditional RGB-D semantic segmentation network, which always suffer from redundant parameters and modules. In this paper, an improved semantic segmentation network PSCNet is designed to reduce redundant parameters and make models easier to implement. Based on the DeepLabv3+ framework, we have improved the original model in three ways, including attention module selection, backbone simplification, and Atrous Spatial Pyramid Pooling (ASPP) module simplification.
              </p>
            </td>
          </tr>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/indoorSeg_ISPRS2021.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/EV-E_9A09QVCiSIps89ER40BwApFmMf3FvPBwBk29_vrrQ?e=2Mg09P">
                <papertitle>Learning deep cross-scale feature propagation for indoor semantic segmentation </papertitle>
              </a>
              <br>
              Linxi Huan, Xianwei Zheng, <strong>Shengjun Tang</strong>, Jianya Gong
              <br>
              ISPRS Journal of Photogrammetry and Remote Sensing, 2021  (SCI, Top)
              <br>
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/EV-E_9A09QVCiSIps89ER40BwApFmMf3FvPBwBk29_vrrQ?e=2Mg09P"> Link </a> 
              <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;"> This paper proposes a deep cross-scale feature propagation network (CSNet), to effectively learn and fuse multi-scale features for robust semantic segmentation of indoor scene images. The proposed CSNet is deployed as an encoder-decoder engine. During encoding, the CSNet propagates contextual information across scales and learn discriminative multi-scale features, which are robust to large object scale variation and indoor occlusion. The decoder of CSNet then adaptively integrates the multi-scale encoded features with fusion supervision at all scales to generate target semantic segmentation prediction. 
              </p>
            </td>
          </tr>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/FITEE_2021.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/EbAatnqld3RAgRJGf-S-f8wBaBLqaPx4gLjfzb_WiXJWhw?e=KK9PNJ">
                <papertitle>A survey on indoor 3D modeling and applications via RGB-D devices </papertitle>
              </a>
              <br>
              Zhilu Yuan, You Li,<strong>Shengjun Tang*</strong>, Ming Li, Renzhong Guo, Weixi Wang
              <br>
              Frontiers of Information Technology & Electronic Engineering, 2021
              <br>
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/EbAatnqld3RAgRJGf-S-f8wBaBLqaPx4gLjfzb_WiXJWhw?e=KK9PNJ"> Link </a> 
              <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;"> In this survey, we provide an overview of recent advances in indoor scene modeling methods, public indoor datasets and libraries which can facilitate experiments and evaluations, and some typical applications using RGB-D devices including indoor localization and emergency evacuation.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/TrSLAM_PERS2020.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/ETmbpTQidZVDpsfKVwdpqzAB-ntaPzFp8WRePHon9bboBQ?e=rYFnBl">
                <papertitle>Trajectory Driftâ€“Compensated Solution of a Stereo RGB-D Mapping System </papertitle>
              </a>
              <br>
             <strong>Shengjun Tang</strong>, Qing Zhu, You Li, Wu Chen, Bo Wu, Renzhong Guo, Xiaoming Li, Chisheng Wang, Weixi Wang
              <br>
              Photogrammetric Engineering & Remote Sensing, 2020
              <br>
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/ETmbpTQidZVDpsfKVwdpqzAB-ntaPzFp8WRePHon9bboBQ?e=rYFnBl"> Link </a> 
              <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;"> In this paper, we describe the trajectory driftâ€“compensated strategy that we designed to eliminate the influence of time drift between sensors, remove the inconsistency between the sequences from various sensors, and thereby generate a coarse-to-fine procedure for robust camera-tracking based on two-dimensionalâ€“3D observations from stereo sensors. 
              </p>
            </td>
          </tr>
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/polelike_RS2019.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.mdpi.com/2072-4292/11/24/2920/pdf">
                <papertitle>Pole-Like Street Furniture Segmentation and Classification in Mobile LiDAR Data by Integrating Multiple Shape-Descriptor Constraints </papertitle>
              </a>
              <br>
             You Li, Weixi Wang, Xiaoming Li, Linfu Xie, Yankun Wang, Renzhong Guo, Wenqun Xiu, <strong>Shengjun Tang*</strong>
              <br>
              Remote Sensing, 2019 (SCI, Top)
              <br>
              <a href="https://www.mdpi.com/2072-4292/11/24/2920/pdf"> Link </a> 
              <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;"> We present a complete paradigm for pole-like street furniture segmentation and classification using mobile LiDAR (light detection and ranging) point cloud. 
              </p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/ComBA_ISPRS2016.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/ESkBuqdEjHpMid6GeXsTuzYB70vNeurvdLG7P_-4AjcQeQ?e=gai1V1">
                <papertitle>Combined adjustment of multi-resolution satellite imagery for improved geo-positioning accuracy</papertitle>
              </a>
              <br>
              <strong>Shengjun Tang</strong>, Bo Wu, Qing Zhu
              <br>
              ISPRS Journal of Photogrammetry and Remote Sensing, 2016  (SCI, Top)
              <br>
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/ESkBuqdEjHpMid6GeXsTuzYB70vNeurvdLG7P_-4AjcQeQ?e=gai1V1"> Link </a> 
              <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;"> This paper presents a combined adjustment approach to integrate multi-source multi-resolution satellite imagery for improved geo-positioning accuracy without the use of ground control points (GCPs). 
              </p>
            </td>
          </tr>
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/Com_ISPRS2015.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="http://www.lsgi.polyu.edu.hk/staff/bo.wu/publications/Wu%20et%20al.%202015_Geometric%20Integration%20of%20HRSI%20and%20LiDAR%20Data.pdf">
                <papertitle>Geometric integration of high-resolution satellite imagery and airborne LiDAR data for improved geopositioning accuracy in metropolitan areas</papertitle>
              </a>
              <br>
              Bo Wu, <strong>Shengjun Tang</strong>, Qing Zhu, Kwan-yuen Tong, Han Hu, Guoyuan Li
              <br>
              ISPRS Journal of Photogrammetry and Remote Sensing, 2015  (SCI, Top)
              <br>
              <a href="http://www.lsgi.polyu.edu.hk/staff/bo.wu/publications/Wu%20et%20al.%202015_Geometric%20Integration%20of%20HRSI%20and%20LiDAR%20Data.pdf"> Link </a> 
              <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;">  Considering HRSI and LiDAR datasets taken from metropolitan areas as a case study, this paper presents a novel approach to the geometric integration of HRSI and LiDAR data to reduce their inconsistencies and improve their geopositioning accuracy. 
              </p>
            </td>
          </tr>
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/para2014.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/EdkBd-LtirBLuFLdy87tv7UBAsyvVgggfexuGrIkwKHr9g?e=7QM6lw	">
                <papertitle>ä¸‰ç»´ GIS ä¸­çš„å‚æ•°åŒ–å»ºæ¨¡æ–¹æ³•(parametric modeling method in three-dimensional GIS) </papertitle>
              </a>
              <br>
              <strong>æ±¤åœ£å›(Shengjun Tang)</strong>, å¼ å¶å»·(Yeting Zhang), è®¸ä¼Ÿå¹³(Weiping Xu), è°¢æ½‡(Xiao Xie), æœ±åº†(Qing Zhu), éŸ©å…ƒåˆ©(Yuanli Han), å´å¼º(Qiang Wu)
              <br>
              æ­¦æ±‰å¤§å­¦å­¦æŠ¥ï¼ˆä¿¡æ¯ç§‘å­¦ç‰ˆï¼‰(Geomatics and Information Science of Wuhan University), 2014
              <br>
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/EdkBd-LtirBLuFLdy87tv7UBAsyvVgggfexuGrIkwKHr9g?e=7QM6lw"> Link </a> 
              <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;"> ä¸ºå®ç°å¤§è§„æ¨¡åœ°å½¢æ™¯è§‚å’Œç²¾ç»†å·¥ç¨‹è®¾æ–½æ¨¡å‹åœ¨ä¸‰ç»´GISä¸­çš„æ— ç¼é›†æˆç®¡ç†,å¹¶æ”¯æŒå·¥ç¨‹è®¾æ–½å…¨ç”Ÿå‘½å‘¨æœŸçš„å…±äº«åº”ç”¨,æå‡ºäº†ä¸€ç§å¯æ ¹æ®è®¾è®¡å‚æ•°è‡ªåŠ¨å»ºç«‹å¤æ‚è®¾æ–½ä¸‰ç»´æ¨¡å‹å¹¶äº¤äº’å¼ç¼–è¾‘ä¿®æ”¹çš„æ–¹æ³•,æ‰©å±•äº†ä¸‰ç»´GISæ•°æ®æ¨¡å‹,å®ç°äº†ä¸‰ç»´å‡ ä½•æ¨¡å‹ä¸å…¶å‚æ•°ä¿¡æ¯çš„æœ‰æœºé›†æˆä¸åŒæ­¥æ›´æ–°,å¹¶ä»¥æ¡¥æ¢æ¨¡å‹çš„æ„å»ºä¸ºä¾‹éªŒè¯äº†è¯¥æ–¹æ³•çš„å¯è¡Œæ€§å’Œæœ‰æ•ˆæ€§ã€‚(To realize the seamless integrated management of large-scale terrain landscape and detailed engineering facility models in 3D GIS, and to support the shared application of the whole lifecycle of engineering facilities, a method is proposed. This method can automatically build complex facility 3D models based on design parameters and allows interactive editing and modification. This method extends the 3D GIS data model, achieves the organic integration and synchronous update of the 3D geometric model and its parameter information. The feasibility and effectiveness of this method have been verified using the construction of a bridge model as an example.)
              </p>
            </td>
          </tr>
          <tr>
        </tbody></table>
	</section>
	<section id="researchprojects">
	 <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Research projects</heading>
              <p>
                <ul>
		    <li style="text-align:justify; text-justify:inter-ideograph;">ä¸­åäººæ°‘å…±å’Œå›½ç§‘å­¦æŠ€æœ¯éƒ¨ï¼Œå›½å®¶é‡ç‚¹ç ”å‘è®¡åˆ’é¡¹ç›®å­è¯¾é¢˜ï¼Œè¶…å¤§åŸå¸‚ç»¿è‰²ä½ç¢³å‘å±•ç›‘æµ‹ä¸è¯Šæ–­ä¼˜åŒ–åº”ç”¨ç¤ºèŒƒ, 2022YFB3903700ï¼Œ 2022-12 è‡³ 2026-11, RMB$248,000,0, PI.</li>
                    <li style="text-align:justify; text-justify:inter-ideograph;">å¹¿ä¸œçœé¢ä¸ŠåŸºé‡‘ï¼Œè”åˆè§†è§‰SLAMä¸æ·±åº¦ç¥ç»ç½‘ç»œçš„å®¤å†…åœºæ™¯è¯­ä¹‰åˆ†ç±»ä¸è‡ªåŠ¨åŒ–å»ºæ¨¡æŠ€æœ¯ï¼Œ2021A1515012574ï¼Œ2021/01/01-2023/12/31ï¼ŒRMB$100,000ï¼ŒPI.</li>
                    <li style="text-align:justify; text-justify:inter-ideograph;">æ·±åœ³å¸‚åŸºç¡€ç ”ç©¶é¢ä¸Šé¡¹ç›®ï¼Œé¢å‘æœºå™¨äººè‡ªä¸»å¯¼èˆªçš„ç±»è„‘è§†è§‰åœºæ™¯ç†è§£ä¸å®æ—¶ä½ç½®è®¡ç®—æ–¹æ³•ï¼ŒJCYJ20210324093012033ï¼Œ2021/10/28-2024/10/27, RMB$600,000ï¼ŒPI.</li>
                    <li style="text-align:justify; text-justify:inter-ideograph;">å›½å®¶è‡ªç„¶ç§‘å­¦åŸºé‡‘å§”é’å¹´åŸºé‡‘ï¼Œå¤šRGB-Dä¼ æ„Ÿå™¨è”åˆçš„åœ¨çº¿å®¤å†…é«˜ç²¾åº¦ä¸‰ç»´æµ‹å›¾æ–¹æ³•ï¼Œ41801392ï¼Œ2019/01/01-2021/12/31, RMB$265,000ï¼ŒPI.</li>
		    <li style="text-align:justify; text-justify:inter-ideograph;">ä¸­å›½åšå£«åç§‘å­¦åŸºé‡‘ï¼Œå¤šå…ƒç‰¹å¾æ··åˆä¼˜åŒ–çš„RGB-Då®¤å†…é«˜ç²¾åº¦ä¸‰ç»´æµ‹å›¾æ–¹æ³•ï¼Œ2018M633133ï¼Œ2018/05/01-2019/09/08,RMB$500,00ï¼ŒPI.</li>	
                    <li style="text-align:justify; text-justify:inter-ideograph;">æ·±åœ³å¸‚ç§‘åˆ›å§”è‡ªç”±æ¢ç´¢é¡¹ç›®ï¼ŒåŸºäºä¾¿æºå¼æ·±åº¦ä¼ æ„Ÿå™¨çš„åŸå¸‚å°é—­/åŠå°é—­ç©ºé—´å¿«é€Ÿä¸‰ç»´æµ‹å›¾æŠ€æœ¯ç ”ç©¶ï¼ŒJCYJ20180305125131482ï¼Œ2019/01/01-2021/12/31, RMB$300,000ï¼ŒPI.</li>
                    <li style="text-align:justify; text-justify:inter-ideograph;">è‡ªç„¶èµ„æºéƒ¨åŸå¸‚è‡ªç„¶èµ„æºç›‘æµ‹ä¸ä»¿çœŸé‡ç‚¹å®éªŒå®¤å¼€æ”¾åŸºé‡‘, åŸºäºå®¤å†…é«˜ç²¾åº¦ä¸‰ç»´æµ‹å›¾çš„BIMå…³é”®éƒ¨ä»¶è‡ªåŠ¨åŒ–é‡å»ºæ–¹æ³•ï¼ŒKF-2019-04-010, 2020/01/17-2021/12/30ï¼ŒRMB$200,000ï¼ŒPI.</li>
                     <li style="text-align:justify; text-justify:inter-ideograph;">æ­¦æ±‰å¤§å­¦æµ‹ç»˜é¥æ„Ÿä¿¡æ¯å·¥ç¨‹å›½å®¶é‡ç‚¹å®éªŒå®¤å¼€æ”¾åŸºé‡‘ï¼Œé›†æˆè§†è§‰ä¸å‡ ä½•ç‰¹å¾çš„RGB-D SLAMæ–¹æ³•ï¼Œ17E04ï¼Œ2018/01/01-2019/12/31, RMB$500,00ï¼ŒPI.</li>
                </ul>
              </p>
            </td>
          </tr>
        </tbody></table>
	</section>
	<section id="awards">
	 <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Awards</heading>
              <p>
                <ul>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">The Best Workshop Presentation Award at the ISPRS Geospatial Week 2023 International Conference for 'Tree-GPT: Modular Large Language Model Expert System for Forest Remote Sensing Image Understanding and Interactive Analysis', 2023.</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">æ·±åœ³å¸‚é«˜å±‚æ¬¡äººæ‰Cç±»(C-Class High-Level Talents in Shenzhen City), 2019.</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">å¹¿ä¸œçœé«˜æ ¡ç§‘æŠ€æˆæœè½¬åŒ–å¤§èµ›ç”µå­ä¿¡æ¯ç»„äºŒç­‰å¥–(Second Prize in the Electronic Information Category of the Guangdong Provincial University Science and Technology Achievement Transformation Competition)<strong>(Rank 2/5)</strong>, 2020.</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">æ·±åœ³å¸‚å—å±±åŒº"2020åˆ›ä¸šä¹‹æ˜Ÿå¤§èµ›"äº’è”ç½‘ä¸ç§»åŠ¨äº’è”ç½‘åˆåˆ›ç»„ä¸€ç­‰å¥–(First Prize in the Internet and Mobile Internet Startup Category of the "2020 Entrepreneurship Star Competition" in Nanshan District, Shenzhen City)<strong>(Rank 1/3)</strong>, 2020.</li>
                </ul>
              </p>
            </td>
          </tr>
        </tbody></table>
	</section>
	<section id="academicservices">
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Academic Service</heading>
              <p> Committee Member:
              </p>
              <p>
                <ul>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">Guest Editor for Remote Sensing, <a href="https://www.mdpi.com/journal/remotesensing/special_issues/RMKV4988H7">Special issue on â€œRemote Sensing for 2D/3D Mappingâ€</a>, From 2023</li>
	            <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">Guest Editor for IEEE JSTAR, <a href="https://www.grss-ieee.org/wp-content/uploads/2023/07/cfp_High-fidelity-Urban-3D-Modeling-and-Scene-Simulation.pdf">Special issue on â€œHigh-fidelity Urban 3D Modeling and Scene Simulationâ€</a>, From 2023</li>
	            <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">Senior Editor for The Photogrammetry Record, From 2023</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">å›½é™…æ•°å­—åœ°çƒåä¼šä¸­å›½è™šæ‹Ÿåœ°ç†ç¯å¢ƒå§”å‘˜ä¼šå§”å‘˜(Member of the China Virtual Geographic Environment Committee of the International Society for Digital Earth), From 2022</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">ä¸­å›½å›¾å­¦å­¦ä¼šBIMä¸“å§”ä¼šå§”å‘˜(Member of the BIM Special Committee of the Chinese Society for Graphics), From 2022</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">ä¸­å›½æµ‹ç»˜å­¦ä¼šä½ç½®æœåŠ¡å·¥å§”ä¼šå§”å‘˜(Member of the Location Service Working Committee of the Chinese Society for Surveying and Mapping), From 2022</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">Expert of ISO Technical Committee 59, From 2022</li>
                </ul>
              </p>
              <p> Journal Reviewer:
              </p>
              <p>
                <ul>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"> ISPRS Journal of Photogrammetry and Remote Sensing</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"> The Photogrammetry Record</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"> IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"> Automation In Construction</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"> International Journal of Applied Earth Observation and Geoinformation</li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"> IEEE Transactions on Industrial Informatics</li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"> IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023)</li>
                </ul>
              </p>
            </td>
          </tr>
        </tbody></table>
	</section>
	 <section id="activites">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Academic Activites</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/asymformer.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
                <Conference information> ISPRS Geospatial Week 2023, to be held in Cairo, Egypt from September 2-7, 2023.</Conference information>
              <br>
              Participants: Shengjun Tang, Weixi Wang,Xiaoming Li, Linfu Xie, Siqi Du, Hongsheng Huang
	      <br>
              Presentations : <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/Ebo7ZRcMkTtNtDzdI1SbgXkB6xMfxKutIzxLtOMLP-vVEQ?e=1JeYlc">  Benchmark of synthetic indoor scenes </a> |  <a href="https://szueducn-my.sharepoint.com/:p:/g/personal/shengjuntang_szu_edu_cn/EfS0IvsLPwNGktjfu6QkA5IBTvmD1oqtTDqXojbK5_zu-g?e=ZwFbWw"> TreeGPT </a> 
              <p></p>
            </td>
          </tr>
		</tbody></table>
	</section>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
        

					
        </tbody></table>
        <p align="center">
          <font size="2">
            Template from <a href="https://shiyujiao.github.io/">shiyu jiao's website</a>.
          </font>
        </p>
      </td>
    </tr>
  </table>
</body>

</html>
