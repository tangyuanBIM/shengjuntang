<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Shengjun Tang</title>
  <meta name="author" content="Shengjun Tang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
        <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ğŸŒ</text></svg>">
        <link href="css/bootstrap.min.css" rel="stylesheet" />
	<link href="css/font-awesome.min.css" rel="stylesheet" type="text/css" />
	<link href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css" />
	<link href="https://fonts.googleapis.com/css?family=Merriweather:400,300,300italic,400italic,700,700italic,900,900italic" rel="stylesheet" type="text/css" /><!-- Plugin CSS -->
	<link href="css/magnific-popup.css" rel="stylesheet" />
	</head>
<body id="page-top">
	<script>
			  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
			  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
			  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
			  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

			  ga('create', 'UA-59354848-1', 'auto');
			  ga('send', 'pageview');
	</script>

	<script language="javascript" type="text/javascript">
		function showHide(shID) {
		   if (document.getElementById(shID)) {
		      if (document.getElementById(shID+'-show').style.display != 'none') {
		         document.getElementById(shID+'-show').style.display = 'none';
		         document.getElementById(shID).style.display = 'block';
		      }
		      else {
		         document.getElementById(shID+'-show').style.display = 'inline';
		         document.getElementById(shID).style.display = 'none';
		      }
		   }
		}
	</script>
	<nav class="navbar navbar-default navbar-fixed-top" id="mainNav">
		<div class="container-fluid">
			<div class="navbar-header">
				<button class="navbar-toggle collapsed" data-target="#bs-example-navbar-collapse-1" data-toggle="collapse" type="button"><span class="sr-only">Toggle navigation</span> Menu</button><a class="navbar-brand page-scroll" href="#page-top" style="color: black;text-decoration: none;font-size:16pt">Shengjun Tang</a>
			</div>
			<div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
				<ul class="nav navbar-nav navbar-top">
					<li><a class="page-scroll" href="#about" style="color: black;text-decoration: none;">About</a></li>
					<li><a class="page-scroll" href="#news" style="color: black;text-decoration: none;">News</a></li>
					<li><a class="page-scroll" href="#publications" style="color: black;text-decoration: none;">Publications</a></li>
					<li><a class="page-scroll" href="#researchprojects" style="color: black;text-decoration: none;">Research Projects</a></li>
					<li><a class="page-scroll" href="#awards" style="color: black;text-decoration: none;">Awards</a></li>
					<li><a class="page-scroll" href="#academicservices" style="color: black;text-decoration: none;">Academic Service</a></li>
				</ul>
			</div>
		</div>
	</nav>
<section id="about">
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Shengjun Tang</name>
              </p>
              <p> Dr. Shengjun Tang is currently an assistant professor/associate researcher at <a href="https://geospatial.szu.edu.cn/kjry/jsxl/2.htm">Shenzhen University</a>. He received his bachelor's and doctoral degrees from <a href="http://liesmars.whu.edu.cn/"> Wuhan University</a>. From 2014 to 2017, he visited <a href="https://www.polyu.edu.hk/lsgi/"> the Department of Land Surveying and Geo-Informatics (LSGI) at The Hong Kong Polytechnic University</a> as a research assistant. After completing his doctoral degree in 2017, he worked as a postdoctoral researcher in the team of Professor Renzhong Guo at <a href="https://geospatial.szu.edu.cn/kjry/jsxl/2.htm">Shenzhen University</a>.
              </p>
              <p>His research interests include Laser and visual simultaneous localization and mapping (SLAM), 3D reconstruction, Scene understanding with deep learning, Virtual Geography and modeling theory, and applications. 
              </p>
              <p style="text-align:center">
                <a href="mailto:shengjuntang@szu.edu.cn">Email</a> &nbsp/&nbsp
                <a href="data/CV_ShengjunTang.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=q-fQQqUAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/ShengjunTang">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="Shengjun%20tang.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="ShengjuntangCircle.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
 </section>
<section id="news">
       <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p>
                <ul>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">05/2023: One papers is submitted to ISPRS Journal</li>
	            <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">03/2023: One papers are submitted to IEEE T-ITS</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">02/2023: Two papers are submitted to IROS 2023</li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">02/2023: One papers is submitted to ICCV 2023</li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">11/2022: One papers is submitted to Automation in Construction</li>
                </ul>
              </p>
            </td>
          </tr>
        </tbody>
        </table>
</section>
<section id="publications">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Publications</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/treemodeling.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="http://www.jors.cn/jrs/ch/reader/download_new_edit_content.aspx?file_no=202210300000001&flag=2">
                <papertitle>é¢å‘è™šæ‹Ÿåœ°ç†ç¯å¢ƒæ„å»ºçš„æ ‘æœ¨æ¨¡å‹é«˜ä¿çœŸä¸‰ç»´é‡å»ºæ–¹æ³• </papertitle>
              </a>
              <br>
              ç‹ä¼Ÿçº, é»„é¸¿ç››,æœæ€é½, ææ™“æ˜, è°¢æ—ç”«, éƒ­ä»å¿ , <strong>æ±¤åœ£å›*</strong>
              <br>
              é¥æ„Ÿå­¦æŠ¥, 2023
              <br>
              <a href="http://www.jors.cn/jrs/ch/reader/download_new_edit_content.aspx?file_no=202210300000001&flag=2"> Link </a> 
              <p></p>
              <p> æœ¬æ–‡é¢å‘è™šæ‹Ÿåœ°ç†ç¯å¢ƒé«˜é€¼çœŸåœºæ™¯æ„å»ºéœ€æ±‚ï¼Œæå‡ºä¸€ç§åŸºäºé«˜ç²¾åº¦æ¿€å…‰æ‰«æç‚¹äº‘æ•°æ®çš„æ ‘æœ¨ä¸‰ç»´æ¨¡å‹é«˜ä¿çœŸä»¿ç”Ÿé‡å»ºæ–¹æ³•ï¼Œå¯å®ç°å½¢æ€ç‰¹å¾ä¿æŒçš„æ ‘æœ¨ä¸‰ç»´æ¨¡å‹è‡ªåŠ¨åŒ–é‡å»ºã€‚     
              </p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/indoorclassification.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="http://ch.whu.edu.cn/article/exportPdf?id=dbb0171b-533f-4d8a-a3d7-67e694032f82">
                <papertitle>è¶…ä½“ç´ éšæœºæ£®æ—ä¸ LSTM ç¥ç»ç½‘ç»œè”åˆä¼˜åŒ–çš„å®¤å†…ç‚¹äº‘é«˜ç²¾åº¦åˆ†ç±»æ–¹æ³• </papertitle>
              </a>
              <br>
              <strong>æ±¤åœ£å›</strong>, å¼ éŸµå©•, ææ™“æ˜, å§šèŒèŒ, å¶è‡´ç…Œ, æäºšé‘«, éƒ­ä»å¿ , ç‹ä¼Ÿçº
              <br>
              æ­¦æ±‰å¤§å­¦å­¦æŠ¥ï¼ˆä¿¡æ¯ç§‘å­¦ç‰ˆï¼‰, 2023
              <br>
              <a href="http://ch.whu.edu.cn/article/exportPdf?id=dbb0171b-533f-4d8a-a3d7-67e694032f82">Link </a>
              <p></p>
              <p> é’ˆå¯¹ç°æœ‰ä¸‰ç»´ç‚¹äº‘æ•°æ®åˆ†å‰²åˆ†ç±»æ–¹æ³•å­˜åœ¨åˆ†ç±»ç›®æ ‡å†…éƒ¨ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œæå‡ºä¸€ç§è¶…ä½“ç´ éšæœºæ£®æ—ä¸é•¿çŸ­æœŸè®°å¿†ç¥ç»ç½‘ç»œ(long short-term memoryï¼ŒLSTM)è”åˆä¼˜åŒ–çš„å®¤å†…ç‚¹äº‘é«˜ç²¾åº¦åˆ†ç±»æ–¹æ³•ã€‚     
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/jag_pond.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://pdf.sciencedirectassets.com/272637/1-s2.0-S1569843223X0003X/1-s2.0-S1569843223001486/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEG0aCXVzLWVhc3QtMSJGMEQCIDb2spK%2F67VOb7fKXQu6A28R%2FRfteSf3Hf8U7wPXBjukAiATFZB7Xj9ZbJ8C0NSC%2FGp6SaZ%2F%2BZfomOFtn5MO0%2BCDHCq8BQiG%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAUaDDA1OTAwMzU0Njg2NSIMhWzSdAUIL%2BGVk1LjKpAFzpyCoIDb4WysFCxf5mFDpTYMqLaFPMpZ64P62%2BJeWN%2BLT1TpKzLSNRaNgjel0oV%2FrZfMcpRqeo6avPA6Ltcz7ifE1XfTNgG5irB2KXZhMLLqd1PRz%2FrM7U5hSG1Pyp88gL0r1HQccMJnFalpkQbqP7fNdzJ0ZsVpRJv0DP%2BslM9gM6fF5zkD%2FyDv4OfzDQoDESIqpJzlJY%2FarESW7WaL5N0KJ1MgRVck%2FEZ1o1jr4xlorWjeMumuc532mQqPk7mpTKxssy53aUYr0pigHtSlCLNkAZSdQnYqjb7OR%2BFrCkETGG1Lyvu0iW2ULdYOfGMlQezISpA8seINAE1MHvpdZQI%2BAyOFL5uVPWuTelJ7t7GE5ZW%2BO%2F40oHzGfUnM7yVdXZo60CEEXrtv1wFxHhJRIHsei%2FtVqE7RXCYNLJum8M3g85t%2FA4AkpkANmCzsH3HEDfaS5DMH2H7cxTEF4BEapdgvKVCoUkb2rg0A%2BZJvLNpkTUVob9JZRFcnKjZ1oDy5PLZwyS219xA7DxPRcROG1gOPg49SA7DiojyC8frOg6hhl%2FErOcRguHbgaVSC3AY5tyJHGnyLNviad0Fb4MEXlAVzCCwK03VpraBsYr5Oxv9ShZMnLE0OaaClGefDS%2F6ckxV91KYJRYSHs1GUQFQkFKXyoEhp6fjObHRd3U1zfAk3QSB%2FV%2B%2FDxaQb%2FodAVvU3O5aIg67byP3iPDcntxmUVsMsSYDNNLbqltyTEWbVwQCtIkS4te85Oo%2B1ieRuaSLWjA4Q%2F1Oulz3tA5naLvAu3dqLY07%2BI5Ub9csEsWBgM0E5DzfFyUQhip7r3CmQ7dYv4gFToGcknMVQKWtMwBkwkP%2FgoHJ61f6FrEp5maAz6Ccw2b3sogY6sgES391tlNmRCrSzKJVA90AqGJRHt5%2BlTMNeobV7JEtrKCkvwJDERzgT2PPCClAkmOn7dlVU%2BudScI9XCRhB%2FBNLXKeqELbrwtfH7jZMnnJeUExhKLOClOxVOQ7jRY5EARBOCmzuWuh5vg6Q1B4XQS8qZUOoMdtuOTIJ%2B8A%2ByUh0leH9AAabPmmBuqpEFqwevTFzxeT0aDICXucd%2BgcKKhDHZz4c%2B92Y8KAfDOcWCsGZHxyx&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20230510T055327Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY3JMUZDOO%2F20230510%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=8ee5e89b67d4c3354e3898e0bcfa81f8c52542ffbd2acc2a441762189f289148&hash=a7a8c4a72931447bc5128a10de27e8eeb9ec0e2a0d54ed10d3d07aabb3f38547&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S1569843223001486&tid=spdf-a022235e-841c-42db-a0d0-c7d388617534&sid=db751d415e6a084c0c3a0af-48fdc60f973dgxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=0c10500155040600545d&rr=7c4fec03db5225e7&cc=jp">
                <papertitle>Unsupervised stepwise extraction of offshore aquaculture ponds using super-resolution hyperspectral images </papertitle>
              </a>
              <br>
	      Siqi Du, Hongsheng Huang, Fan He, Heng Luo, Yumeng Yin, Xiaoming Li, Linfu Xie, Renzhong Guo,  <strong>Shengjun Tang*</strong>
              <br>
              International Journal of Applied Earth Observation and Geoinformation, 2023 (SCI, Top)
              <br>
              <a href="https://pdf.sciencedirectassets.com/272637/1-s2.0-S1569843223X0003X/1-s2.0-S1569843223001486/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEG0aCXVzLWVhc3QtMSJGMEQCIDb2spK%2F67VOb7fKXQu6A28R%2FRfteSf3Hf8U7wPXBjukAiATFZB7Xj9ZbJ8C0NSC%2FGp6SaZ%2F%2BZfomOFtn5MO0%2BCDHCq8BQiG%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAUaDDA1OTAwMzU0Njg2NSIMhWzSdAUIL%2BGVk1LjKpAFzpyCoIDb4WysFCxf5mFDpTYMqLaFPMpZ64P62%2BJeWN%2BLT1TpKzLSNRaNgjel0oV%2FrZfMcpRqeo6avPA6Ltcz7ifE1XfTNgG5irB2KXZhMLLqd1PRz%2FrM7U5hSG1Pyp88gL0r1HQccMJnFalpkQbqP7fNdzJ0ZsVpRJv0DP%2BslM9gM6fF5zkD%2FyDv4OfzDQoDESIqpJzlJY%2FarESW7WaL5N0KJ1MgRVck%2FEZ1o1jr4xlorWjeMumuc532mQqPk7mpTKxssy53aUYr0pigHtSlCLNkAZSdQnYqjb7OR%2BFrCkETGG1Lyvu0iW2ULdYOfGMlQezISpA8seINAE1MHvpdZQI%2BAyOFL5uVPWuTelJ7t7GE5ZW%2BO%2F40oHzGfUnM7yVdXZo60CEEXrtv1wFxHhJRIHsei%2FtVqE7RXCYNLJum8M3g85t%2FA4AkpkANmCzsH3HEDfaS5DMH2H7cxTEF4BEapdgvKVCoUkb2rg0A%2BZJvLNpkTUVob9JZRFcnKjZ1oDy5PLZwyS219xA7DxPRcROG1gOPg49SA7DiojyC8frOg6hhl%2FErOcRguHbgaVSC3AY5tyJHGnyLNviad0Fb4MEXlAVzCCwK03VpraBsYr5Oxv9ShZMnLE0OaaClGefDS%2F6ckxV91KYJRYSHs1GUQFQkFKXyoEhp6fjObHRd3U1zfAk3QSB%2FV%2B%2FDxaQb%2FodAVvU3O5aIg67byP3iPDcntxmUVsMsSYDNNLbqltyTEWbVwQCtIkS4te85Oo%2B1ieRuaSLWjA4Q%2F1Oulz3tA5naLvAu3dqLY07%2BI5Ub9csEsWBgM0E5DzfFyUQhip7r3CmQ7dYv4gFToGcknMVQKWtMwBkwkP%2FgoHJ61f6FrEp5maAz6Ccw2b3sogY6sgES391tlNmRCrSzKJVA90AqGJRHt5%2BlTMNeobV7JEtrKCkvwJDERzgT2PPCClAkmOn7dlVU%2BudScI9XCRhB%2FBNLXKeqELbrwtfH7jZMnnJeUExhKLOClOxVOQ7jRY5EARBOCmzuWuh5vg6Q1B4XQS8qZUOoMdtuOTIJ%2B8A%2ByUh0leH9AAabPmmBuqpEFqwevTFzxeT0aDICXucd%2BgcKKhDHZz4c%2B92Y8KAfDOcWCsGZHxyx&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20230510T055327Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY3JMUZDOO%2F20230510%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=8ee5e89b67d4c3354e3898e0bcfa81f8c52542ffbd2acc2a441762189f289148&hash=a7a8c4a72931447bc5128a10de27e8eeb9ec0e2a0d54ed10d3d07aabb3f38547&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S1569843223001486&tid=spdf-a022235e-841c-42db-a0d0-c7d388617534&sid=db751d415e6a084c0c3a0af-48fdc60f973dgxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=0c10500155040600545d&rr=7c4fec03db5225e7&cc=jp"> Link </a> 
              <p></p>
              <p> In this paper, we proposed an unsupervised aquaculture ponds extraction method based on hyperspectral imagery super-resolution, feature fusion and stepwise extraction strategy. 
              </p>
            </td>
          </tr>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/treesegJSTAR.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10040735">
                <papertitle>An Individual Tree Segmentation Method from Mobile Mapping Point Clouds Based on Improved 3D Morphological Analysis</papertitle>
              </a>
              <br>
             Weixi Wang, Yuhang Fan, You Li, Xiaoming Li, <strong>Shengjun Tang*</strong>
              <br>
              IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2023
              <br>
              <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10040735"> Link </a> 
              <p></p>
              <p> We propose a method based on improved 3D morphological analysis for extracting street trees from mobile laser scanner (MLS) point clouds. 
              </p>
            </td>
          </tr>


        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/AutoBIM_AIC2022.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/Ed6Z8EtiPuJCnmUxBtFyRh4BybZIVqXjb0ooq_NgcrynmA?e=k4vy8o">
                <papertitle>BIM generation from 3D point clouds by combining 3D deep learning and improved morphological approach</papertitle>
              </a>
              <br>
             <strong> Shengjun Tang</strong>, Xiaoming Li, Xianwei Zheng, Bo Wu, Weixi Wang, Yunjie Zhang
              <br>
              Automation In Construction, 2022 (SCI, Top)
              <br>
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/Ed6Z8EtiPuJCnmUxBtFyRh4BybZIVqXjb0ooq_NgcrynmA?e=k4vy8o"> Link </a> 
              <p></p>
              <p> This paper presents a novel parametric modeling method for reconstructing semantic volumetric building interiors from the unstructured point cloud of a building. Unlike existing partitioning-based methods, our proposed method overcomes the limitations by providing a flexible framework for combining 3D Deep Learning and an improved morphological approach for inverse BIM modeling. 
              </p>
            </td>
          </tr>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/superV_RS2022.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.mdpi.com/2072-4292/14/6/1516/pdf">
                <papertitle>A Supervoxel-Based Random Forest Method for Robust and Effective Airborne LiDAR Point Cloud Classification</papertitle>
              </a>
              <br>
             Lingfeng Liao, <strong> Shengjun Tang*</strong>, Jianghai Liao, Xiaoming Li, Weixi Wang, Yaxin Li, Renzhong Guo
              <br>
              Remote Sensing, 2022 (SCI, Top)
              <br>
              <a href="https://www.mdpi.com/2072-4292/14/6/1516/pdf"> Link </a> 
              <p></p>
              <p> In this paper, we propose a robust and effective point cloud classification approach that integrates point cloud supervoxels and their locally convex connected patches into a random forest classifier, which effectively improves the point cloud feature calculation accuracy and reduces the computational cost. 
              </p>
            </td>
          </tr>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/pscnet_isprsannals2022.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://isprs-annals.copernicus.org/articles/V-1-2022/129/2022/isprs-annals-V-1-2022-129-2022.pdf">
                <papertitle>PSCNET: EFFICIENT RGB-D SEMANTIC SEGMENTATION PARALLEL NETWORK BASED ON SPATIAL AND CHANNEL ATTENTION </papertitle>
              </a>
              <br>
              Siqi Du, <strong>Shengjun Tang*</strong>, Weixi Wang, Xiaoming Li, Yonghua Lu, Renzhong Guo
              <br>
              ISPRS Annals, 2022
              <br>
              <a href="https://isprs-annals.copernicus.org/articles/V-1-2022/129/2022/isprs-annals-V-1-2022-129-2022.pdf"> Link </a> 
              <p></p>
              <p> RGB-D semantic segmentation algorithm is a key technology for indoor semantic map construction. The traditional RGB-D semantic segmentation network, which always suffer from redundant parameters and modules. In this paper, an improved semantic segmentation network PSCNet is designed to reduce redundant parameters and make models easier to implement. Based on the DeepLabv3+ framework, we have improved the original model in three ways, including attention module selection, backbone simplification, and Atrous Spatial Pyramid Pooling (ASPP) module simplification.
              </p>
            </td>
          </tr>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/indoorSeg_ISPRS2021.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/EV-E_9A09QVCiSIps89ER40BwApFmMf3FvPBwBk29_vrrQ?e=2Mg09P">
                <papertitle>Learning deep cross-scale feature propagation for indoor semantic segmentation </papertitle>
              </a>
              <br>
              Linxi Huan, Xianwei Zheng, <strong>Shengjun Tang</strong>, Jianya Gong
              <br>
              ISPRS Journal of Photogrammetry and Remote Sensing, 2021  (SCI, Top)
              <br>
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/EV-E_9A09QVCiSIps89ER40BwApFmMf3FvPBwBk29_vrrQ?e=2Mg09P"> Link </a> 
              <p></p>
              <p> This paper proposes a deep cross-scale feature propagation network (CSNet), to effectively learn and fuse multi-scale features for robust semantic segmentation of indoor scene images. The proposed CSNet is deployed as an encoder-decoder engine. During encoding, the CSNet propagates contextual information across scales and learn discriminative multi-scale features, which are robust to large object scale variation and indoor occlusion. The decoder of CSNet then adaptively integrates the multi-scale encoded features with fusion supervision at all scales to generate target semantic segmentation prediction. 
              </p>
            </td>
          </tr>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/FITEE_2021.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/EbAatnqld3RAgRJGf-S-f8wBaBLqaPx4gLjfzb_WiXJWhw?e=KK9PNJ">
                <papertitle>A survey on indoor 3D modeling and applications via RGB-D devices </papertitle>
              </a>
              <br>
              Zhilu Yuan, You Li,<strong>Shengjun Tang*</strong>, Ming Li, Renzhong Guo, Weixi Wang
              <br>
              Frontiers of Information Technology & Electronic Engineering, 2021
              <br>
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/EbAatnqld3RAgRJGf-S-f8wBaBLqaPx4gLjfzb_WiXJWhw?e=KK9PNJ"> Link </a> 
              <p></p>
              <p> In this survey, we provide an overview of recent advances in indoor scene modeling methods, public indoor datasets and libraries which can facilitate experiments and evaluations, and some typical applications using RGB-D devices including indoor localization and emergency evacuation.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/TrSLAM_PERS2020.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/ETmbpTQidZVDpsfKVwdpqzAB-ntaPzFp8WRePHon9bboBQ?e=rYFnBl">
                <papertitle>Trajectory Driftâ€“Compensated Solution of a Stereo RGB-D Mapping System </papertitle>
              </a>
              <br>
             <strong>Shengjun Tang</strong>, Qing Zhu, You Li, Wu Chen, Bo Wu, Renzhong Guo, Xiaoming Li, Chisheng Wang, Weixi Wang
              <br>
              Photogrammetric Engineering & Remote Sensing, 2020
              <br>
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/ETmbpTQidZVDpsfKVwdpqzAB-ntaPzFp8WRePHon9bboBQ?e=rYFnBl"> Link </a> 
              <p></p>
              <p> In this paper, we describe the trajectory driftâ€“compensated strategy that we designed to eliminate the influence of time drift between sensors, remove the inconsistency between the sequences from various sensors, and thereby generate a coarse-to-fine procedure for robust camera-tracking based on two-dimensionalâ€“3D observations from stereo sensors. 
              </p>
            </td>
          </tr>
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/polelike_RS2019.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.mdpi.com/2072-4292/11/24/2920/pdf">
                <papertitle>Pole-Like Street Furniture Segmentation and Classification in Mobile LiDAR Data by Integrating Multiple Shape-Descriptor Constraints </papertitle>
              </a>
              <br>
             You Li, Weixi Wang, Xiaoming Li, Linfu Xie, Yankun Wang, Renzhong Guo, Wenqun Xiu, <strong>Shengjun Tang*</strong>
              <br>
              Remote Sensing, 2019 (SCI, Top)
              <br>
              <a href="https://www.mdpi.com/2072-4292/11/24/2920/pdf"> Link </a> 
              <p></p>
              <p> We present a complete paradigm for pole-like street furniture segmentation and classification using mobile LiDAR (light detection and ranging) point cloud. 
              </p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/ComBA_ISPRS2016.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/ESkBuqdEjHpMid6GeXsTuzYB70vNeurvdLG7P_-4AjcQeQ?e=gai1V1">
                <papertitle>Combined adjustment of multi-resolution satellite imagery for improved geo-positioning accuracy</papertitle>
              </a>
              <br>
              <strong>Shengjun Tang</strong>, Bo Wu, Qing Zhu
              <br>
              ISPRS Journal of Photogrammetry and Remote Sensing, 2016  (SCI, Top)
              <br>
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/ESkBuqdEjHpMid6GeXsTuzYB70vNeurvdLG7P_-4AjcQeQ?e=gai1V1"> Link </a> 
              <p></p>
              <p> This paper presents a combined adjustment approach to integrate multi-source multi-resolution satellite imagery for improved geo-positioning accuracy without the use of ground control points (GCPs). 
              </p>
            </td>
          </tr>
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/Com_ISPRS2015.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="http://www.lsgi.polyu.edu.hk/staff/bo.wu/publications/Wu%20et%20al.%202015_Geometric%20Integration%20of%20HRSI%20and%20LiDAR%20Data.pdf">
                <papertitle>Geometric integration of high-resolution satellite imagery and airborne LiDAR data for improved geopositioning accuracy in metropolitan areas</papertitle>
              </a>
              <br>
              Bo Wu, <strong>Shengjun Tang</strong>, Qing Zhu, Kwan-yuen Tong, Han Hu, Guoyuan Li
              <br>
              ISPRS Journal of Photogrammetry and Remote Sensing, 2015  (SCI, Top)
              <br>
              <a href="http://www.lsgi.polyu.edu.hk/staff/bo.wu/publications/Wu%20et%20al.%202015_Geometric%20Integration%20of%20HRSI%20and%20LiDAR%20Data.pdf"> Link </a> 
              <p></p>
              <p>  Considering HRSI and LiDAR datasets taken from metropolitan areas as a case study, this paper presents a novel approach to the geometric integration of HRSI and LiDAR data to reduce their inconsistencies and improve their geopositioning accuracy. 
              </p>
            </td>
          </tr>
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/para2014.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/EdkBd-LtirBLuFLdy87tv7UBAsyvVgggfexuGrIkwKHr9g?e=7QM6lw	">
                <papertitle>ä¸‰ç»´ GIS ä¸­çš„å‚æ•°åŒ–å»ºæ¨¡æ–¹æ³• </papertitle>
              </a>
              <br>
              <strong>æ±¤åœ£å›</strong>, å¼ å¶å»·, è®¸ä¼Ÿå¹³, è°¢æ½‡, æœ±åº†, éŸ©å…ƒåˆ©, å´å¼º
              <br>
              æ­¦æ±‰å¤§å­¦å­¦æŠ¥ï¼ˆä¿¡æ¯ç§‘å­¦ç‰ˆï¼‰, 2014
              <br>
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/EdkBd-LtirBLuFLdy87tv7UBAsyvVgggfexuGrIkwKHr9g?e=7QM6lw"> Link </a> 
              <p></p>
              <p> ä¸ºå®ç°å¤§è§„æ¨¡åœ°å½¢æ™¯è§‚å’Œç²¾ç»†å·¥ç¨‹è®¾æ–½æ¨¡å‹åœ¨ä¸‰ç»´GISä¸­çš„æ— ç¼é›†æˆç®¡ç†,å¹¶æ”¯æŒå·¥ç¨‹è®¾æ–½å…¨ç”Ÿå‘½å‘¨æœŸçš„å…±äº«åº”ç”¨,æå‡ºäº†ä¸€ç§å¯æ ¹æ®è®¾è®¡å‚æ•°è‡ªåŠ¨å»ºç«‹å¤æ‚è®¾æ–½ä¸‰ç»´æ¨¡å‹å¹¶äº¤äº’å¼ç¼–è¾‘ä¿®æ”¹çš„æ–¹æ³•,æ‰©å±•äº†ä¸‰ç»´GISæ•°æ®æ¨¡å‹,å®ç°äº†ä¸‰ç»´å‡ ä½•æ¨¡å‹ä¸å…¶å‚æ•°ä¿¡æ¯çš„æœ‰æœºé›†æˆä¸åŒæ­¥æ›´æ–°,å¹¶ä»¥æ¡¥æ¢æ¨¡å‹çš„æ„å»ºä¸ºä¾‹éªŒè¯äº†è¯¥æ–¹æ³•çš„å¯è¡Œæ€§å’Œæœ‰æ•ˆæ€§ã€‚
              </p>
            </td>
          </tr>
          <tr>
        </tbody></table>
	<section id="researchprojects">
	 <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Research projects</heading>
              <p>
                <ul>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">ä¸­åäººæ°‘å…±å’Œå›½ç§‘å­¦æŠ€æœ¯éƒ¨ï¼Œå›½å®¶é‡ç‚¹ç ”å‘è®¡åˆ’é¡¹ç›®å­è¯¾é¢˜ï¼Œè¶…å¤§åŸå¸‚ç»¿è‰²ä½ç¢³å‘å±•ç›‘æµ‹ä¸è¯Šæ–­ä¼˜åŒ–åº”ç”¨ç¤ºèŒƒ, 2022YFB3903700ï¼Œ 2022-12 è‡³ 2026-11, RMB$248,000,0, PI.</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">å¹¿ä¸œçœé¢ä¸ŠåŸºé‡‘ï¼Œè”åˆè§†è§‰SLAMä¸æ·±åº¦ç¥ç»ç½‘ç»œçš„å®¤å†…åœºæ™¯è¯­ä¹‰åˆ†ç±»ä¸è‡ªåŠ¨åŒ–å»ºæ¨¡æŠ€æœ¯ï¼Œ2021A1515012574ï¼Œ2021/01/01-2023/12/31ï¼ŒRMB$100,000ï¼ŒPI.</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">æ·±åœ³å¸‚åŸºç¡€ç ”ç©¶é¢ä¸Šé¡¹ç›®ï¼Œé¢å‘æœºå™¨äººè‡ªä¸»å¯¼èˆªçš„ç±»è„‘è§†è§‰åœºæ™¯ç†è§£ä¸å®æ—¶ä½ç½®è®¡ç®—æ–¹æ³•ï¼ŒJCYJ20210324093012033ï¼Œ2021/10/28-2024/10/27, RMB$600,000ï¼ŒPI.</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">å›½å®¶è‡ªç„¶ç§‘å­¦åŸºé‡‘å§”é’å¹´åŸºé‡‘ï¼Œå¤šRGB-Dä¼ æ„Ÿå™¨è”åˆçš„åœ¨çº¿å®¤å†…é«˜ç²¾åº¦ä¸‰ç»´æµ‹å›¾æ–¹æ³•ï¼Œ41801392ï¼Œ2019/01/01-2021/12/31, RMB$265,000ï¼ŒPI.</li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">ä¸­å›½åšå£«åç§‘å­¦åŸºé‡‘ï¼Œå¤šå…ƒç‰¹å¾æ··åˆä¼˜åŒ–çš„RGB-Då®¤å†…é«˜ç²¾åº¦ä¸‰ç»´æµ‹å›¾æ–¹æ³•ï¼Œ2018M633133ï¼Œ2018/05/01-2019/09/08,RMB$500,00ï¼ŒPI.</li>	
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">æ·±åœ³å¸‚ç§‘åˆ›å§”è‡ªç”±æ¢ç´¢é¡¹ç›®ï¼ŒåŸºäºä¾¿æºå¼æ·±åº¦ä¼ æ„Ÿå™¨çš„åŸå¸‚å°é—­/åŠå°é—­ç©ºé—´å¿«é€Ÿä¸‰ç»´æµ‹å›¾æŠ€æœ¯ç ”ç©¶ï¼ŒJCYJ20180305125131482ï¼Œ2019/01/01-2021/12/31, RMB$300,000ï¼ŒPI.</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">è‡ªç„¶èµ„æºéƒ¨åŸå¸‚è‡ªç„¶èµ„æºç›‘æµ‹ä¸ä»¿çœŸé‡ç‚¹å®éªŒå®¤å¼€æ”¾åŸºé‡‘, åŸºäºå®¤å†…é«˜ç²¾åº¦ä¸‰ç»´æµ‹å›¾çš„BIMå…³é”®éƒ¨ä»¶è‡ªåŠ¨åŒ–é‡å»ºæ–¹æ³•ï¼ŒKF-2019-04-010, 2020/01/17-2021/12/30ï¼ŒRMB$200,000ï¼ŒPI.</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">æ­¦æ±‰å¤§å­¦æµ‹ç»˜é¥æ„Ÿä¿¡æ¯å·¥ç¨‹å›½å®¶é‡ç‚¹å®éªŒå®¤å¼€æ”¾åŸºé‡‘ï¼Œé›†æˆè§†è§‰ä¸å‡ ä½•ç‰¹å¾çš„RGB-D SLAMæ–¹æ³•ï¼Œ17E04ï¼Œ2018/01/01-2019/12/31, RMB$500,00ï¼ŒPI.</li>
                </ul>
              </p>
            </td>
          </tr>
        </tbody></table>
	</section>
	<section id="awards">
	 <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Awards</heading>
              <p>
                <ul>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">æ·±åœ³å¸‚é«˜å±‚æ¬¡äººæ‰Cç±», 2019.</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">å¹¿ä¸œçœé«˜æ ¡ç§‘æŠ€æˆæœè½¬åŒ–å¤§èµ›ç”µå­ä¿¡æ¯ç»„äºŒç­‰å¥–<strong>(Rank 2/5)</strong>, 2020.</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">æ·±åœ³å¸‚å—å±±åŒº"2020åˆ›ä¸šä¹‹æ˜Ÿå¤§èµ›"äº’è”ç½‘ä¸ç§»åŠ¨äº’è”ç½‘åˆåˆ›ç»„ä¸€ç­‰å¥–<strong>(Rank 1/3)</strong>, 2020.</li>
                </ul>
              </p>
            </td>
          </tr>
        </tbody></table>
	</section>
	<section id="academicservices">
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Academic Service</heading>
              <p> Committee Member/Reviewer:
              </p>
              <p>
                <ul>
	            <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">Senior Editor for The Photogrammetry Record, From 2023</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">ä¸­å›½è™šæ‹Ÿåœ°ç†ç¯å¢ƒå§”å‘˜ä¼šå§”å‘˜, From 2022</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">ä¸­å›½å›¾å­¦å­¦ä¼šBIMä¸“å§”ä¼šå§”å‘˜, From 2022</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">ä¸­å›½æµ‹ç»˜å­¦ä¼šä½ç½®æœåŠ¡å·¥å§”ä¼šå§”å‘˜, From 2022</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">Expert of ISO Technical Committee 59, From 2022</li>
                </ul>
              </p>
              <p> Journal Reviewer:
              </p>
              <p>
                <ul>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"> ISPRS Journal of Photogrammetry and Remote Sensing</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"> The Photogrammetry Record</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"> IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"> Automation In Construction</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"> International Journal of Applied Earth Observation and Geoinformation</li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"> IEEE Transactions on Industrial Informatics</li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"> IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023)</li>
                </ul>
              </p>
            </td>
          </tr>
        </tbody></table>
	</section>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
        

					
        </tbody></table>
        <p align="center">
          <font size="2">
            Template from <a href="https://shiyujiao.github.io/">shiyu jiao's website</a>.
          </font>
        </p>
      </td>
    </tr>
  </table>
</body>

</html>
