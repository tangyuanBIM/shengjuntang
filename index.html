<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Shengjun Tang</title>
  
  <meta name="author" content="Shengjun Tang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ğŸŒ</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Shengjun Tang</name>
              </p>
              <p> Dr. Shengjun Tang is currently an Assistant Professor/Associate Researcher at <a href="https://www.szu.edu.cn/">Shenzhen University</a>. Prior to this, he worked as a Research Assistant at <a href="https://www.polyu.edu.hk/lsgi/"> the Department of Land Surveying and Geo-Informatics (LSGI) at The Hong Kong Polytechnic University</a>, and as a Postdoctoral Researcher in the team of Academician Renzhong Guo at Shenzhen University.
	      </p>
              <p>His research interests include 3D reconstruction and scene understanding, laser and visual simultaneous localization and mapping (SLAM), Virtual Geography and modeling theory, and applications. 
              </p>
              <p style="text-align:center">
                <a href="mailto:shengjuntang@szu.edu.cn">Email</a> &nbsp/&nbsp
                <a href="data/CV_ShengjunTang.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=q-fQQqUAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/shiyujiao">Github</a> &nbsp/&nbsp
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/Shengjun%20tang.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/ShengjuntangCircle.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

<!--         <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p>
                <ul>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">07/2022: I start an internship at Tencent</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">06/2022: One paper accepted to TPAMI 2022 </li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">03/2022: One paper accepted to CVPR 2022</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">01/2022: I start an internship at Meta Reality Lab Research</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">12/2021: One paper accepted to TPAMI 2022</li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">06/2021: I am nominated as one of the CVPR 2021 outstanding reviewer</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">03/2021: One paper accepted to CVPR 2021</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">03/2020: One paper accepted to CVPR 2020</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">11/2019: One paper accepted to AAAI 2020</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">09/2019: One paper accepted to NeurIPS 2019</li>

                </ul>
              </p>
            </td>
          </tr>
        </tbody>
        </table> -->



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Publications</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/treemodeling.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="http://www.jors.cn/jrs/ch/reader/download_new_edit_content.aspx?file_no=202210300000001&flag=2">
                <papertitle>é¢å‘è™šæ‹Ÿåœ°ç†ç¯å¢ƒæ„å»ºçš„æ ‘æœ¨æ¨¡å‹é«˜ä¿çœŸä¸‰ç»´é‡å»ºæ–¹æ³• </papertitle>
              </a>
              <br>
              ç‹ä¼Ÿçº, é»„é¸¿ç››,æœæ€é½, ææ™“æ˜, è°¢æ—ç”«, éƒ­ä»å¿ , <strong>æ±¤åœ£å›*</strong>
              <br>
              é¥æ„Ÿå­¦æŠ¥, 2023
              <br>
              <a href="http://www.jors.cn/jrs/ch/reader/download_new_edit_content.aspx?file_no=202210300000001&flag=2"> paper </a>  /
              <a href="https://github.com/shiyujiao/HighlyAccurate"> code </a>
              <p></p>
              <p> æœ¬æ–‡é¢å‘è™šæ‹Ÿåœ°ç†ç¯å¢ƒé«˜é€¼çœŸåœºæ™¯æ„å»ºéœ€æ±‚ï¼Œæå‡ºä¸€ç§åŸºäºé«˜ç²¾åº¦æ¿€å…‰æ‰«æç‚¹äº‘æ•°æ®çš„æ ‘æœ¨ä¸‰ç»´æ¨¡å‹é«˜ä¿çœŸä»¿ç”Ÿé‡å»ºæ–¹æ³•ï¼Œå¯å®ç°å½¢æ€ç‰¹å¾ä¿æŒçš„æ ‘æœ¨ä¸‰ç»´æ¨¡å‹è‡ªåŠ¨åŒ–é‡å»ºã€‚é¦–å…ˆæå‡ºåŸºäºéª¨æ¶çš„æ ‘æœ¨æ¨¡å‹å‚æ•°åŒ–é‡æ„æ–¹æ³•ï¼Œé€šè¿‡å¹¿ä¹‰åœ†æŸ±ä½“æ‹Ÿåˆå®ç°æ ‘æå‡ ä½•å½¢çŠ¶çš„æŠ½å–ï¼Œå¹¶æ ¹æ®æ ‘æœ¨ç”Ÿé•¿å‚æ•°å¯¹æ ‘å¹²ã€ä¸»è¦ææ¡ã€ç»†å°ææ¡æ¨¡å‹ä»¥åŠæ ‘å† ç­‰è¦ç´ è¿›è¡Œåˆ†çº§æå–ï¼›å…¶æ¬¡è€ƒè™‘æ ‘æœ¨ä¸åŒéƒ¨ä½ç²¾ç»†åŒ–å»ºæ¨¡è¦æ±‚ï¼Œæå‡ºæ³Šæ¾æ„ç½‘ä¸å‚æ•°æ‹Ÿåˆèåˆçš„æ ‘æœ¨å‡ ä½•æ¨¡å‹ç²¾ç»†åŒ–é‡å»ºæ–¹æ³•ï¼Œè¿›è€ŒåŸºäºè¾¹ç•Œçº¦æŸæ¡ä»¶å®ç°æ ‘å¹²ä¸æ ‘ææ¨¡å‹çš„ç²¾å‡†æ‹¼æ¥ä¸èåˆï¼›æœ€åé‡‡ç”¨é¡¾åŠæ ‘æœ¨ç»“æ„çš„çº¹ç†å±•å¼€æ–¹æ³•ï¼Œå¯¹å¤šå±‚çº§æ ‘æœ¨æå¹²è¿›è¡Œçº¹ç†è‡ªåŠ¨æ˜ å°„è´´å›¾ï¼Œå®ç°é«˜ä¿çœŸçš„æ ‘æœ¨æ¨¡å‹ä¸‰ç»´é‡å»ºã€‚ç»å®éªŒéªŒè¯ï¼ŒåŸºäºèƒŒåŒ…å¼æˆ–ç«™ç‚¹å¼è·å–çš„æ¿€å…‰ç‚¹äº‘ï¼Œæœ¬æ–¹æ³•å¯ç”Ÿæˆå½¢æ€ç‰¹å¾é«˜ä¿çœŸçš„ç²¾ç»†åŒ–ä¸‰ç»´æ ‘æœ¨æ¨¡å‹ï¼Œæ¨¡å‹æ•´ä½“å‡ ä½•è¯¯å·®ä¼˜äº10cmï¼Œæ ‘å¹²æ¨¡å‹è¯¯å·®ä¼˜äº3cmã€‚ä¸”åœ¨ç›¸åŒæ•°æ®æ¡ä»¶ä¸‹ï¼Œä¸å‡ ç§ä¸»æµæ ‘æœ¨å»ºæ¨¡æ–¹æ³•å¯¹æ¯”ï¼Œæœ¬æ–¹æ³•å¯¹æ ‘æœ¨ä¸‰ç»´å½¢æ€å’ŒçœŸå®çº¹ç†çš„è¿˜åŸåº¦ç¨‹åº¦æœ€é«˜ã€‚
		      
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/3DOF.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2203.14148.pdf">
                <papertitle>Accurate 3-DoF Camera Geo-Localization via Ground-to-Satellite Image Matching </papertitle>
              </a>
              <br>
              <strong>Yujiao Shi </strong>, Xin Yu, Liu Liu, Dylan Campbell, Piotr Koniusz, and Hongdong Li
              <br>
              TPAMI, 2022
              <br>
              <a href="https://arxiv.org/pdf/2203.14148.pdf"> paper </a>  /
              <a href="https://github.com/shiyujiao/IBL"> code </a>
              <p></p>
              <p> We propose projective transform, which (1) compliments polar transform to achieve better coarse localization performance and (2) provides a novel handcrafted method to accurately localize query camera on its matching satellite image.
              </p>
            </td>
          </tr>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/Sat2Str.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2103.01623.pdf">
                <papertitle>Geometry-Guided Street-View Panorama Synthesis from Satellite Imagery </papertitle>
              </a>
              <br>
              <strong>Yujiao Shi </strong>, Dylan Campbell, Xin Yu, and Hongdong Li
              <br>
              TPAMI, 2022
              <br>
              <a href="https://arxiv.org/pdf/2103.01623.pdf"> paper </a>  /
              <a href="https://github.com/shiyujiao/Sat2StrPanoramaSynthesis"> code </a>
              <p></p>
              <p> Satellite to street-view panorama synthesis, implicit satellite image height map estimation.
              </p>
            </td>
          </tr>


        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/SVNVS.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Shi_Self-Supervised_Visibility_Learning _for_Novel_View_Synthesis_CVPR_2021_paper.pdf">
                <papertitle>Self-Supervised Visibility Learning for Novel View Synthesis </papertitle>
              </a>
              <br>
              <strong>Yujiao Shi </strong>, Hongdong Li, and Xin Yu
              <br>
              CVPR, 2021
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Shi_Self-Supervised_Visibility_Learning_for_Novel_View_Synthesis_CVPR_2021_paper.pdf"> paper </a>  /
              <a href="https://github.com/shiyujiao/SVNVS"> code </a>
              <p></p>
              <p> We estimate target-view depth and source-view visibility in an end-to-end manner.
              </p>
            </td>
          </tr>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/DSM.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Shi_Where_Am_I_Looking_At_Joint_Location_and_Orientation_Estimation_CVPR_2020_paper.pdf">
                <papertitle>Where am I looking at? Joint Location and Orientation Estimation by Cross-View Matching </papertitle>
              </a>
              <br>
              <strong>Yujiao Shi </strong>, Xin Yu, Dylan Campbell, and Hongdong Li
              <br>
              CVPR, 2020
              <br>
              <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Shi_Where_Am_I_Looking_At_Joint_Location_and_Orientation_Estimation_CVPR_2020_paper.pdf"> paper </a>  /
              <a href="https://github.com/shiyujiao/cross_view_localization_DSM"> code </a>
              <p></p>
              <p> The first 3-DoF camera pose estimation framework via ground-to-satellite image matching.
              </p>
            </td>
          </tr>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/CVFT.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://ojs.aaai.org//index.php/AAAI/article/view/6875">
                <papertitle>Optimal Feature Transport for Cross-View Image Geo-Localization </papertitle>
              </a>
              <br>
              <strong>Yujiao Shi </strong>, Xin Yu, Liu Liu, Tong Zhang, and Hongdong Li
              <br>
              AAAI, 2020
              <br>
              <a href="https://ojs.aaai.org//index.php/AAAI/article/view/6875"> paper </a>  /
              <a href="https://github.com/shiyujiao/cross_view_localization_CVFT"> code </a>
              <p></p>
              <p> Motivated by optimal transport, we invent a cross-view feature transport module to bridge the cross-view domain gap.
              </p>
            </td>
          </tr>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/SAFA.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://proceedings.neurips.cc/paper/2019/file/ba2f0015122a5955f8b3a50240fb91b2-Paper.pdf">
                <papertitle>Spatial-Aware Feature Aggregation for Cross-View Image based Geo-Localization </papertitle>
              </a>
              <br>
              <strong>Yujiao Shi </strong>, Liu Liu, Xin Yu, and Hongdong Li
              <br>
              NeurIPS, 2019
              <br>
              <a href="https://proceedings.neurips.cc/paper/2019/file/ba2f0015122a5955f8b3a50240fb91b2-Paper.pdf"> paper </a>  /
              <a href="https://github.com/shiyujiao/cross_view_localization_SAFA"> code </a>
              <p></p>
              <p> A polar transform is introduced to bridge the ground-and-satellite domain gap, which significantly boosts the state-of-the-art cross-view localization performance.
              </p>
            </td>
          </tr>


        </tbody></table>

				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Academic Service</heading>
              <p> Conference Program Committee Member/Reviewer:
              </p>
              <p>
                <ul>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021, 2022</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">International Conference on Computer Vision (ICCV), 2021</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">European Conference on Computer Vision (ECCV), 2020, 2022</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">Asian Conference on Computer Vision (ACCV), 2020, 2022</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">Neural Information Processing Systems (NeurIPS), 2022</li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">The International Conference on Learning Representations (ICLR), 2021, 2022</li> 
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">AAAI Conference on Artificial Intelligence (AAAI), 2021, 2022, 2023</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">International Joint Conference on Artificial Intelligence (IJCAI), 2020, 2022</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">International Conference on 3D Vision (3DV), 2022</li>

                </ul>
              </p>
              <p> Journal Reviewer:
              </p>
              <p>
                <ul>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">International Journal of Computer Vision (IJCV)</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">IEEE Transactions on Image Processing (TIP)</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">The IEEE Robotics and Automation Letters (RAL)</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">IEEE Transactions on Artificial Intelligence</li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">IEEE Transactions on Geoscience and Remote Sensing</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">ISPRS Journal of Photogrammetry and Remote Sensing</li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">International Journal of Digital Earth</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">Knowledge-Based Systems</li>
                    
                </ul>
              </p>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
        

					
        </tbody></table>
        <p align="center">
          <font size="2">
            Template from <a href="https://shiyujiao.github.io/">shiyu jiao's website</a>.
          </font>
        </p>
      </td>
    </tr>
  </table>
</body>

</html>
