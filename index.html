<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-RJFVJNLM3P"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-RJFVJNLM3P');
</script>
  <title>Shengjun Tang</title>
  <meta name="author" content="Shengjun Tang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
        <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
        <link href="css/bootstrap.min.css" rel="stylesheet" />
	<link href="css/font-awesome.min.css" rel="stylesheet" type="text/css" />
	<link href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css" />
	<link href="https://fonts.googleapis.com/css?family=Merriweather:400,300,300italic,400italic,700,700italic,900,900italic" rel="stylesheet" type="text/css" /><!-- Plugin CSS -->
	<link href="css/magnific-popup.css" rel="stylesheet" />
	</head>
<body id="page-top">
	<script>
			  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
			  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
			  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
			  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

			  ga('create', 'UA-59354848-1', 'auto');
			  ga('send', 'pageview');
	</script>

	<script language="javascript" type="text/javascript">
		function showHide(shID) {
		   if (document.getElementById(shID)) {
		      if (document.getElementById(shID+'-show').style.display != 'none') {
		         document.getElementById(shID+'-show').style.display = 'none';
		         document.getElementById(shID).style.display = 'block';
		      }
		      else {
		         document.getElementById(shID+'-show').style.display = 'inline';
		         document.getElementById(shID).style.display = 'none';
		      }
		   }
		}
	</script>
	<nav class="navbar navbar-default navbar-fixed-top" id="mainNav">
		<div class="container-fluid">
			<div class="navbar-header">
				<button class="navbar-toggle collapsed" data-target="#bs-example-navbar-collapse-1" data-toggle="collapse" type="button"><span class="sr-only">Toggle navigation</span> Menu</button><a class="navbar-brand page-scroll" href="#page-top" style="color: black;text-decoration: none;font-size:16pt">Homepage</a>
			</div>
			<div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
				<ul class="nav navbar-nav navbar-top">
					<li><a class="page-scroll" href="#about" style="color: black;text-decoration: none;">About</a></li>
					<li><a class="page-scroll" href="#news" style="color: black;text-decoration: none;">News</a></li>
					<li><a class="page-scroll" href="#publications" style="color: black;text-decoration: none;">Publications</a></li>
					<li><a class="page-scroll" href="#researchprojects" style="color: black;text-decoration: none;">Research Projects</a></li>
					<li><a class="page-scroll" href="#awards" style="color: black;text-decoration: none;">Awards</a></li>
					<li><a class="page-scroll" href="#academicservices" style="color: black;text-decoration: none;">Academic Service</a></li>
					<li><a class="page-scroll" href="#activites" style="color: black;text-decoration: none;">Academic Activites</a></li>
				</ul>
			</div>
		</div>
	</nav><br/>
<section id="about">
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Shengjun Tang (Tommy)</name>
              </p>
              <p style="text-align:justify; text-justify:inter-ideograph;"> <a href="https://geospatial.szu.edu.cn/info/1006/3518.htm">Dr. Shengjun Tang</a> is currently an associate researcher at <a href="https://geospatial.szu.edu.cn/kjry/jsxl/2.htm">Shenzhen University</a>. He received his bachelor's and doctoral degrees from <a href="http://liesmars.whu.edu.cn/"> Wuhan University</a>. From 2014 to 2017, he visited <a href="https://www.polyu.edu.hk/lsgi/"> the Department of Land Surveying and Geo-Informatics (LSGI) at The Hong Kong Polytechnic University</a> as a research assistant. After completing his doctoral degree in 2017, he worked as a postdoctoral researcher in the team of Professor Renzhong Guo at <a href="https://geospatial.szu.edu.cn/kjry/jsxl/2.htm">Shenzhen University</a>.
              </p>
              <p style="text-align:justify; text-justify:inter-ideograph;" >His research interests include Laser and visual simultaneous localization and mapping (SLAM), 3D reconstruction, Scene understanding with deep learning, Virtual Geography and modeling theory, and applications.<br/>  <a href="mailto:shengjuntang@szu.edu.cn">(Email:shengjuntang@szu.edu.cn)</a>
              </p>
              <p style="text-align:center">
                <a href="mailto:shengjuntang@szu.edu.cn">Email</a> &nbsp/&nbsp
                <a href="data/CV_ShengjunTang.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=q-fQQqUAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/ShengjunTang">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="Shengjun%20tang.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="shengjuntangcircle1.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
 </section>
<section id="news">
       <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p>
                <ul>
		<li style="list-style-position:inside;margin:0;padding:0;text-align:left;">11/2023: Congratulations, a paper titled "Branching the Limits: Leveraging  Robust 3D Tree Reconstruction from Incomplete Laser Point Clouds ‚ÄØ" has been accepted in International Journal of Applied Earth Observation and Geoinformation</li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">11/2023: Congratulations, a paper titled "Backpack LiDAR-based SLAM with Multiple Ground Constraints for Multi-storey Indoor Mapping" has been accepted in IEEE Transactions on Geoscience and Remote Sensing</li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">10/2023: Congratulations, we have published a paper titled "Robust Calibration of Vehicle Solid-state Lidar-Camera Perception System Using Line-Weighted Correspondences in Natural Environments" in IEEE Transactions on Intelligent Transportation Systems</li>
	            <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">09/2023: Congratulations, our team won the Best Workshop Presentation Award at the ISPRS Geospatial Week 2023 International Conference for 'Tree-GPT: Modular Large Language Model Expert System for Forest Remote Sensing Image Understanding and Interactive Analysis'.</a></li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">09/2023: We organize a special issue in Remote Sensing. <a href="https://www.grss-ieee.org/wp-content/uploads/2023/07/cfp_High-fidelity-Urban-3D-Modeling-and-Scene-Simulation.pdf">Special issue on Remote Sensing for 2D/3D Mapping‚Äù</a>. Welcomes contributions from researchers and experts in the field. Submission Date End: 31 Mar 2024.</li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">08/2023: Congratulations, we have published a paper titled "Skeleton-guided generation of synthetic noisy point clouds from as-built BIM to improve indoor scene understanding" in the Automation in Construction journal. The publicly available 3D point cloud simulation dataset provided by the paper can be accessed at the following link:<a href="https://szueducn-my.sharepoint.com/:f:/g/personal/shengjuntang_szu_edu_cn/EgpOf3leEiFOjtfQTl-k7GgBougQGuQFcHDAbgtBZmVZ5w?e=rPPlot"> BIMSyn Dataset.</a></li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">07/2023: We organize a special issue in IEEE JSTAR.<a href="https://www.grss-ieee.org/wp-content/uploads/2023/07/cfp_High-fidelity-Urban-3D-Modeling-and-Scene-Simulation.pdf">Special issue on ‚ÄúHigh-fidelity Urban 3D Modeling and Scene Simulation‚Äù</a>. Welcomes contributions from researchers and experts in the field. Submission Date Start: 01 Oct 2023; Submission Date End: 30 Apr 2024.</li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">05/2023: One papers is submitted to ISPRS Journal</li>
	            <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">03/2023: One papers are submitted to IEEE T-ITS</li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">11/2022: One papers is submitted to Automation in Construction</li>
                </ul>
              </p>
            </td>
          </tr>
        </tbody>
        </table>
</section>
<section id="publications">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Publications</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/bplidar.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="">
                <papertitle>Backpack LiDAR-based SLAM with Multiple Ground Constraints for Multi-storey Indoor Mapping(Accepted)</papertitle>
              </a>
              <br>
              Baoding Zhou, Haoquan Mo, <strong>Shengjun Tang*</strong>, Xing Zhang, and Qingquan Li
              <br>
              International Journal of Applied Earth Observation and Geoinformation, 2023
              <br>
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/EblKSK0s8t9MrBvAmoHee9sBe29UyyoYpNNovXTqzIQRlg?e=wWnPAg"> Link </a> 
              <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;"> This paper proposed a SLAM method base on multiple ground constraints pose optimization (MGCPO) which uses a backpack LiDAR system. The proposed method includes two novel modules. The first, a regression analysis-based scenarios recognition (RASR) module provides a reference for the construction of ground constraints. The second, based on different scene detection results, the MGCPO module constrains the sensor pose using the floor plane to reduce localization errors and effectively decrease loop closure detection errors.</a>
              </p>
            </td>
          </tr>
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/bplidar.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="">
                <papertitle>Backpack LiDAR-based SLAM with Multiple Ground Constraints for Multi-storey Indoor Mapping(Accepted)</papertitle>
              </a>
              <br>
              Baoding Zhou, Haoquan Mo, <strong>Shengjun Tang*</strong>, Xing Zhang, and Qingquan Li
              <br>
              IEEE Transactions on Geoscience and Remote Sensing, 2023
              <br>
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/EblKSK0s8t9MrBvAmoHee9sBe29UyyoYpNNovXTqzIQRlg?e=wWnPAg"> Link </a> 
              <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;"> This paper proposed a SLAM method base on multiple ground constraints pose optimization (MGCPO) which uses a backpack LiDAR system. The proposed method includes two novel modules. The first, a regression analysis-based scenarios recognition (RASR) module provides a reference for the construction of ground constraints. The second, based on different scene detection results, the MGCPO module constrains the sensor pose using the floor plane to reduce localization errors and effectively decrease loop closure detection errors.</a>
              </p>
            </td>
          </tr>
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/asymformer.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://browse.arxiv.org/pdf/2309.14065.pdf">
                <papertitle>AsymFormer: Asymmetrical Cross-Modal Representation Learning for Mobile Platform Real-Time RGB-D Semantic Segmentation (arXiv preprint)</papertitle>
              </a>
              <br>
              Siqi Du, Weixi Wang, Renzhong Guo and <strong>Shengjun Tang*</strong>
              <br>
              Submitted to ICRA 2024
              <br>
              <a href="https://browse.arxiv.org/pdf/2309.14065.pdf"> Link </a> |  <a href="https://github.com/Fourier7754/AsymFormer"> Code </a> 
              <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;">  In this work, we propose AsymFormer, a novel network for real-time RGB-D semantic segmentation, which targets the minimization of superfluous parameters by optimizing the distribution of computational resources and introduces an asymmetrical backbone to allow for the effective fusion of multimodal features..</a>
              </p>
            </td>
          </tr>
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/calib.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/EeEA--97IxxGjppxKUhz1j8B36Z6Gn8V8JXrEu4HUzf_Wg?e=nq5M6M">
                <papertitle>Robust Calibration of Vehicle Solid-state Lidar-Camera Perception System Using Line-Weighted Correspondences in Natural Environments(Accepted)</papertitle>
              </a>
              <br>
              <strong>Shengjun Tang</strong>, Yunqi Feng, Junjie Huang, Xiaoming Li, Zhihan Lv, Yuhong Feng, and Weixi Wang
              <br>
              IEEE Transactions on Intelligent Transportation Systems, 2023
              <br>
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/EblKSK0s8t9MrBvAmoHee9sBe29UyyoYpNNovXTqzIQRlg?e=wWnPAg"> Link </a> 
              <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;"> In this paper, we present a novel approach for robustly calibrating the extrinsic parameters of a solid-state(SS) lidar-camera system in a natural environment.  We conducted a performance study to compare our proposed method against existing targetless calibration methods on various natural scenarios. The experimental results demonstrate that our proposed method achieves higher robustness, accuracy, and consistency, making it suitable for real-world applications.</a>
              </p>
            </td>
          </tr>
        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/Syndatasets_AIC2023.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/EeEA--97IxxGjppxKUhz1j8B36Z6Gn8V8JXrEu4HUzf_Wg?e=nq5M6M">
                <papertitle>Skeleton-guided generation of synthetic noisy point clouds from as-built BIM to improve indoor scene understanding</papertitle>
              </a>
              <br>
              <strong>Shengjun Tang</strong>, Hongsheng Huang, Yunjie Zhang, Mengmeng Yao, Xiaoming Li, Linfu Xie, and Weixi Wang
              <br>
              Automation in Construction, 2023
              <br>
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/EeEA--97IxxGjppxKUhz1j8B36Z6Gn8V8JXrEu4HUzf_Wg?e=nq5M6M"> Link </a> | <a href="https://szueducn-my.sharepoint.com/:f:/g/personal/shengjuntang_szu_edu_cn/EmAdsrCIxrtEl21hocr8NMwB11DDjCJJM9ATA8Yzt1uNLQ?e=Ze0r5j"> Datasets </a> 
              <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;"> In this study, a fully automatic method to generate synthetic noisy point clouds from as-built building information modeling (BIM) models is presented and it assesses the potential of these synthetic point clouds to improve deep neural network training. All simulation datasets are publicly available, including original BIM models, full synthetic point clouds, and point clouds after IHPR processing, accessible via the following link: <a href="https://szueducn-my.sharepoint.com/:f:/g/personal/shengjuntang_szu_edu_cn/EgpOf3leEiFOjtfQTl-k7GgBougQGuQFcHDAbgtBZmVZ5w?e=rPPlot"> BIMSyn Dataset.</a>
              </p>
            </td>
          </tr>
        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/treemodeling.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="http://www.jors.cn/jrs/ch/reader/download_new_edit_content.aspx?file_no=202210300000001&flag=2">
                <papertitle>Èù¢ÂêëËôöÊãüÂú∞ÁêÜÁéØÂ¢ÉÊûÑÂª∫ÁöÑÊ†ëÊú®Ê®°ÂûãÈ´ò‰øùÁúü‰∏âÁª¥ÈáçÂª∫ÊñπÊ≥ï(A highly realistic 3D reconstruction method for tree models created for virtual geographic environments) </papertitle>
              </a>
              <br>
              Áéã‰ºüÁé∫(Weixi Wang), ÈªÑÈ∏øÁõõ(Hongsheng Huang),ÊùúÊÄùÈΩê(Siqi Du), ÊùéÊôìÊòé(Xiaoming Li), Ë∞¢ÊûóÁî´(Linfu Xie), ÈÉ≠‰ªÅÂø†(Renzhong Guo), <strong>Ê±§Âú£Âêõ(Shengjun Tang)*</strong>
              <br>
              ÈÅ•ÊÑüÂ≠¶Êä•(Journal of Remote Sensing), 2023
              <br>
              <a href="http://www.jors.cn/jrs/ch/reader/download_new_edit_content.aspx?file_no=202210300000001&flag=2"> Link </a> 
              <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;"> Êú¨ÊñáÈù¢ÂêëËôöÊãüÂú∞ÁêÜÁéØÂ¢ÉÈ´òÈÄºÁúüÂú∫ÊôØÊûÑÂª∫ÈúÄÊ±ÇÔºåÊèêÂá∫‰∏ÄÁßçÂü∫‰∫éÈ´òÁ≤æÂ∫¶ÊøÄÂÖâÊâ´ÊèèÁÇπ‰∫ëÊï∞ÊçÆÁöÑÊ†ëÊú®‰∏âÁª¥Ê®°ÂûãÈ´ò‰øùÁúü‰ªøÁîüÈáçÂª∫ÊñπÊ≥ïÔºåÂèØÂÆûÁé∞ÂΩ¢ÊÄÅÁâπÂæÅ‰øùÊåÅÁöÑÊ†ëÊú®‰∏âÁª¥Ê®°ÂûãËá™Âä®ÂåñÈáçÂª∫„ÄÇ(In this paper, we propose a bionic reconstruction method for 3D tree models based on high-precision laser scanning point cloud data for building realistic scenes in virtual geographic environments, which enables the automated reconstruction of 3D tree models at multiple levels of detail while preserving morphological features.)     
              </p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/indoorclassification.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="http://ch.whu.edu.cn/article/exportPdf?id=dbb0171b-533f-4d8a-a3d7-67e694032f82">
                <papertitle>Ë∂Ö‰ΩìÁ¥†ÈöèÊú∫Ê£ÆÊûó‰∏é LSTM Á•ûÁªèÁΩëÁªúËÅîÂêà‰ºòÂåñÁöÑÂÆ§ÂÜÖÁÇπ‰∫ëÈ´òÁ≤æÂ∫¶ÂàÜÁ±ªÊñπÊ≥ï(A High-Precision Indoor Point Cloud Classification Method Jointly Optimized by Super Voxel Random Forest and LSTM Neural Network) </papertitle>
              </a>
              <br>
              <strong>Ê±§Âú£Âêõ(Shengjun Tang)</strong>, Âº†ÈüµÂ©ï(Yunjie Zhang), ÊùéÊôìÊòé(Xiaoming Li), ÂßöËêåËêå(Mengmeng Yao), Âè∂Ëá¥ÁÖå(Zhihuang Ye), Êùé‰∫öÈë´(Yaxin Li), ÈÉ≠‰ªÅÂø†(Renzhong Guo), Áéã‰ºüÁé∫(Weixi Wang)
              <br>
              Ê≠¶Ê±âÂ§ßÂ≠¶Â≠¶Êä•Ôºà‰ø°ÊÅØÁßëÂ≠¶ÁâàÔºâ(Geomatics and Information Science of Wuhan University), 2023
              <br>
              <a href="http://ch.whu.edu.cn/article/exportPdf?id=dbb0171b-533f-4d8a-a3d7-67e694032f82">Link </a>
              <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;"> ÈíàÂØπÁé∞Êúâ‰∏âÁª¥ÁÇπ‰∫ëÊï∞ÊçÆÂàÜÂâ≤ÂàÜÁ±ªÊñπÊ≥ïÂ≠òÂú®ÂàÜÁ±ªÁõÆÊ†áÂÜÖÈÉ®‰∏ç‰∏ÄËá¥ÁöÑÈóÆÈ¢òÔºåÊèêÂá∫‰∏ÄÁßçË∂Ö‰ΩìÁ¥†ÈöèÊú∫Ê£ÆÊûó‰∏éÈïøÁü≠ÊúüËÆ∞ÂøÜÁ•ûÁªèÁΩëÁªú(long short-term memoryÔºåLSTM)ËÅîÂêà‰ºòÂåñÁöÑÂÆ§ÂÜÖÁÇπ‰∫ëÈ´òÁ≤æÂ∫¶ÂàÜÁ±ªÊñπÊ≥ï„ÄÇ  (In response to the problem of internal inconsistency in the classification targets of existing 3D point cloud data segmentation methods, we propose a high-precision indoor point cloud classification method that jointly optimizes super-voxel random forests and Long Short-Term Memory (LSTM) neural networks.)   
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/jag_pond.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://pdf.sciencedirectassets.com/272637/1-s2.0-S1569843223X0003X/1-s2.0-S1569843223001486/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEG0aCXVzLWVhc3QtMSJGMEQCIDb2spK%2F67VOb7fKXQu6A28R%2FRfteSf3Hf8U7wPXBjukAiATFZB7Xj9ZbJ8C0NSC%2FGp6SaZ%2F%2BZfomOFtn5MO0%2BCDHCq8BQiG%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAUaDDA1OTAwMzU0Njg2NSIMhWzSdAUIL%2BGVk1LjKpAFzpyCoIDb4WysFCxf5mFDpTYMqLaFPMpZ64P62%2BJeWN%2BLT1TpKzLSNRaNgjel0oV%2FrZfMcpRqeo6avPA6Ltcz7ifE1XfTNgG5irB2KXZhMLLqd1PRz%2FrM7U5hSG1Pyp88gL0r1HQccMJnFalpkQbqP7fNdzJ0ZsVpRJv0DP%2BslM9gM6fF5zkD%2FyDv4OfzDQoDESIqpJzlJY%2FarESW7WaL5N0KJ1MgRVck%2FEZ1o1jr4xlorWjeMumuc532mQqPk7mpTKxssy53aUYr0pigHtSlCLNkAZSdQnYqjb7OR%2BFrCkETGG1Lyvu0iW2ULdYOfGMlQezISpA8seINAE1MHvpdZQI%2BAyOFL5uVPWuTelJ7t7GE5ZW%2BO%2F40oHzGfUnM7yVdXZo60CEEXrtv1wFxHhJRIHsei%2FtVqE7RXCYNLJum8M3g85t%2FA4AkpkANmCzsH3HEDfaS5DMH2H7cxTEF4BEapdgvKVCoUkb2rg0A%2BZJvLNpkTUVob9JZRFcnKjZ1oDy5PLZwyS219xA7DxPRcROG1gOPg49SA7DiojyC8frOg6hhl%2FErOcRguHbgaVSC3AY5tyJHGnyLNviad0Fb4MEXlAVzCCwK03VpraBsYr5Oxv9ShZMnLE0OaaClGefDS%2F6ckxV91KYJRYSHs1GUQFQkFKXyoEhp6fjObHRd3U1zfAk3QSB%2FV%2B%2FDxaQb%2FodAVvU3O5aIg67byP3iPDcntxmUVsMsSYDNNLbqltyTEWbVwQCtIkS4te85Oo%2B1ieRuaSLWjA4Q%2F1Oulz3tA5naLvAu3dqLY07%2BI5Ub9csEsWBgM0E5DzfFyUQhip7r3CmQ7dYv4gFToGcknMVQKWtMwBkwkP%2FgoHJ61f6FrEp5maAz6Ccw2b3sogY6sgES391tlNmRCrSzKJVA90AqGJRHt5%2BlTMNeobV7JEtrKCkvwJDERzgT2PPCClAkmOn7dlVU%2BudScI9XCRhB%2FBNLXKeqELbrwtfH7jZMnnJeUExhKLOClOxVOQ7jRY5EARBOCmzuWuh5vg6Q1B4XQS8qZUOoMdtuOTIJ%2B8A%2ByUh0leH9AAabPmmBuqpEFqwevTFzxeT0aDICXucd%2BgcKKhDHZz4c%2B92Y8KAfDOcWCsGZHxyx&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20230510T055327Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY3JMUZDOO%2F20230510%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=8ee5e89b67d4c3354e3898e0bcfa81f8c52542ffbd2acc2a441762189f289148&hash=a7a8c4a72931447bc5128a10de27e8eeb9ec0e2a0d54ed10d3d07aabb3f38547&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S1569843223001486&tid=spdf-a022235e-841c-42db-a0d0-c7d388617534&sid=db751d415e6a084c0c3a0af-48fdc60f973dgxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=0c10500155040600545d&rr=7c4fec03db5225e7&cc=jp">
                <papertitle>Unsupervised stepwise extraction of offshore aquaculture ponds using super-resolution hyperspectral images </papertitle>
              </a>
              <br>
	      Siqi Du, Hongsheng Huang, Fan He, Heng Luo, Yumeng Yin, Xiaoming Li, Linfu Xie, Renzhong Guo,  <strong>Shengjun Tang*</strong>
              <br>
              International Journal of Applied Earth Observation and Geoinformation, 2023 (SCI, Top)
              <br>
              <a href="https://pdf.sciencedirectassets.com/272637/1-s2.0-S1569843223X0003X/1-s2.0-S1569843223001486/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEG0aCXVzLWVhc3QtMSJGMEQCIDb2spK%2F67VOb7fKXQu6A28R%2FRfteSf3Hf8U7wPXBjukAiATFZB7Xj9ZbJ8C0NSC%2FGp6SaZ%2F%2BZfomOFtn5MO0%2BCDHCq8BQiG%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAUaDDA1OTAwMzU0Njg2NSIMhWzSdAUIL%2BGVk1LjKpAFzpyCoIDb4WysFCxf5mFDpTYMqLaFPMpZ64P62%2BJeWN%2BLT1TpKzLSNRaNgjel0oV%2FrZfMcpRqeo6avPA6Ltcz7ifE1XfTNgG5irB2KXZhMLLqd1PRz%2FrM7U5hSG1Pyp88gL0r1HQccMJnFalpkQbqP7fNdzJ0ZsVpRJv0DP%2BslM9gM6fF5zkD%2FyDv4OfzDQoDESIqpJzlJY%2FarESW7WaL5N0KJ1MgRVck%2FEZ1o1jr4xlorWjeMumuc532mQqPk7mpTKxssy53aUYr0pigHtSlCLNkAZSdQnYqjb7OR%2BFrCkETGG1Lyvu0iW2ULdYOfGMlQezISpA8seINAE1MHvpdZQI%2BAyOFL5uVPWuTelJ7t7GE5ZW%2BO%2F40oHzGfUnM7yVdXZo60CEEXrtv1wFxHhJRIHsei%2FtVqE7RXCYNLJum8M3g85t%2FA4AkpkANmCzsH3HEDfaS5DMH2H7cxTEF4BEapdgvKVCoUkb2rg0A%2BZJvLNpkTUVob9JZRFcnKjZ1oDy5PLZwyS219xA7DxPRcROG1gOPg49SA7DiojyC8frOg6hhl%2FErOcRguHbgaVSC3AY5tyJHGnyLNviad0Fb4MEXlAVzCCwK03VpraBsYr5Oxv9ShZMnLE0OaaClGefDS%2F6ckxV91KYJRYSHs1GUQFQkFKXyoEhp6fjObHRd3U1zfAk3QSB%2FV%2B%2FDxaQb%2FodAVvU3O5aIg67byP3iPDcntxmUVsMsSYDNNLbqltyTEWbVwQCtIkS4te85Oo%2B1ieRuaSLWjA4Q%2F1Oulz3tA5naLvAu3dqLY07%2BI5Ub9csEsWBgM0E5DzfFyUQhip7r3CmQ7dYv4gFToGcknMVQKWtMwBkwkP%2FgoHJ61f6FrEp5maAz6Ccw2b3sogY6sgES391tlNmRCrSzKJVA90AqGJRHt5%2BlTMNeobV7JEtrKCkvwJDERzgT2PPCClAkmOn7dlVU%2BudScI9XCRhB%2FBNLXKeqELbrwtfH7jZMnnJeUExhKLOClOxVOQ7jRY5EARBOCmzuWuh5vg6Q1B4XQS8qZUOoMdtuOTIJ%2B8A%2ByUh0leH9AAabPmmBuqpEFqwevTFzxeT0aDICXucd%2BgcKKhDHZz4c%2B92Y8KAfDOcWCsGZHxyx&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20230510T055327Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY3JMUZDOO%2F20230510%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=8ee5e89b67d4c3354e3898e0bcfa81f8c52542ffbd2acc2a441762189f289148&hash=a7a8c4a72931447bc5128a10de27e8eeb9ec0e2a0d54ed10d3d07aabb3f38547&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S1569843223001486&tid=spdf-a022235e-841c-42db-a0d0-c7d388617534&sid=db751d415e6a084c0c3a0af-48fdc60f973dgxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=0c10500155040600545d&rr=7c4fec03db5225e7&cc=jp"> Link </a> 
              <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;"> In this paper, we proposed an unsupervised aquaculture ponds extraction method based on hyperspectral imagery super-resolution, feature fusion and stepwise extraction strategy. 
              </p>
            </td>
          </tr>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/treesegJSTAR.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10040735">
                <papertitle>An Individual Tree Segmentation Method from Mobile Mapping Point Clouds Based on Improved 3D Morphological Analysis</papertitle>
              </a>
              <br>
             Weixi Wang, Yuhang Fan, You Li, Xiaoming Li, <strong>Shengjun Tang*</strong>
              <br>
              IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2023
              <br>
              <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10040735"> Link </a> 
              <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;"> We propose a method based on improved 3D morphological analysis for extracting street trees from mobile laser scanner (MLS) point clouds. 
              </p>
            </td>
          </tr>


        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/AutoBIM_AIC2022.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/Ed6Z8EtiPuJCnmUxBtFyRh4BybZIVqXjb0ooq_NgcrynmA?e=k4vy8o">
                <papertitle>BIM generation from 3D point clouds by combining 3D deep learning and improved morphological approach</papertitle>
              </a>
              <br>
             <strong> Shengjun Tang</strong>, Xiaoming Li, Xianwei Zheng, Bo Wu, Weixi Wang, Yunjie Zhang
              <br>
              Automation In Construction, 2022 (SCI, Top)
              <br>
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/Ed6Z8EtiPuJCnmUxBtFyRh4BybZIVqXjb0ooq_NgcrynmA?e=k4vy8o"> Link </a> 
              <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;"> This paper presents a novel parametric modeling method for reconstructing semantic volumetric building interiors from the unstructured point cloud of a building. Unlike existing partitioning-based methods, our proposed method overcomes the limitations by providing a flexible framework for combining 3D Deep Learning and an improved morphological approach for inverse BIM modeling. 
              </p>
            </td>
          </tr>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/superV_RS2022.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.mdpi.com/2072-4292/14/6/1516/pdf">
                <papertitle>A Supervoxel-Based Random Forest Method for Robust and Effective Airborne LiDAR Point Cloud Classification</papertitle>
              </a>
              <br>
             Lingfeng Liao, <strong> Shengjun Tang*</strong>, Jianghai Liao, Xiaoming Li, Weixi Wang, Yaxin Li, Renzhong Guo
              <br>
              Remote Sensing, 2022 (SCI, Top)
              <br>
              <a href="https://www.mdpi.com/2072-4292/14/6/1516/pdf"> Link </a> 
              <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;"> In this paper, we propose a robust and effective point cloud classification approach that integrates point cloud supervoxels and their locally convex connected patches into a random forest classifier, which effectively improves the point cloud feature calculation accuracy and reduces the computational cost. 
              </p>
            </td>
          </tr>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/pscnet_isprsannals2022.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://isprs-annals.copernicus.org/articles/V-1-2022/129/2022/isprs-annals-V-1-2022-129-2022.pdf">
                <papertitle>PSCNET: EFFICIENT RGB-D SEMANTIC SEGMENTATION PARALLEL NETWORK BASED ON SPATIAL AND CHANNEL ATTENTION </papertitle>
              </a>
              <br>
              Siqi Du, <strong>Shengjun Tang*</strong>, Weixi Wang, Xiaoming Li, Yonghua Lu, Renzhong Guo
              <br>
              ISPRS Annals, 2022
              <br>
              <a href="https://isprs-annals.copernicus.org/articles/V-1-2022/129/2022/isprs-annals-V-1-2022-129-2022.pdf"> Link </a> 
              <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;"> RGB-D semantic segmentation algorithm is a key technology for indoor semantic map construction. The traditional RGB-D semantic segmentation network, which always suffer from redundant parameters and modules. In this paper, an improved semantic segmentation network PSCNet is designed to reduce redundant parameters and make models easier to implement. Based on the DeepLabv3+ framework, we have improved the original model in three ways, including attention module selection, backbone simplification, and Atrous Spatial Pyramid Pooling (ASPP) module simplification.
              </p>
            </td>
          </tr>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/indoorSeg_ISPRS2021.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/EV-E_9A09QVCiSIps89ER40BwApFmMf3FvPBwBk29_vrrQ?e=2Mg09P">
                <papertitle>Learning deep cross-scale feature propagation for indoor semantic segmentation </papertitle>
              </a>
              <br>
              Linxi Huan, Xianwei Zheng, <strong>Shengjun Tang</strong>, Jianya Gong
              <br>
              ISPRS Journal of Photogrammetry and Remote Sensing, 2021  (SCI, Top)
              <br>
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/EV-E_9A09QVCiSIps89ER40BwApFmMf3FvPBwBk29_vrrQ?e=2Mg09P"> Link </a> 
              <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;"> This paper proposes a deep cross-scale feature propagation network (CSNet), to effectively learn and fuse multi-scale features for robust semantic segmentation of indoor scene images. The proposed CSNet is deployed as an encoder-decoder engine. During encoding, the CSNet propagates contextual information across scales and learn discriminative multi-scale features, which are robust to large object scale variation and indoor occlusion. The decoder of CSNet then adaptively integrates the multi-scale encoded features with fusion supervision at all scales to generate target semantic segmentation prediction. 
              </p>
            </td>
          </tr>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/FITEE_2021.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/EbAatnqld3RAgRJGf-S-f8wBaBLqaPx4gLjfzb_WiXJWhw?e=KK9PNJ">
                <papertitle>A survey on indoor 3D modeling and applications via RGB-D devices </papertitle>
              </a>
              <br>
              Zhilu Yuan, You Li,<strong>Shengjun Tang*</strong>, Ming Li, Renzhong Guo, Weixi Wang
              <br>
              Frontiers of Information Technology & Electronic Engineering, 2021
              <br>
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/EbAatnqld3RAgRJGf-S-f8wBaBLqaPx4gLjfzb_WiXJWhw?e=KK9PNJ"> Link </a> 
              <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;"> In this survey, we provide an overview of recent advances in indoor scene modeling methods, public indoor datasets and libraries which can facilitate experiments and evaluations, and some typical applications using RGB-D devices including indoor localization and emergency evacuation.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/TrSLAM_PERS2020.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/ETmbpTQidZVDpsfKVwdpqzAB-ntaPzFp8WRePHon9bboBQ?e=rYFnBl">
                <papertitle>Trajectory Drift‚ÄìCompensated Solution of a Stereo RGB-D Mapping System </papertitle>
              </a>
              <br>
             <strong>Shengjun Tang</strong>, Qing Zhu, You Li, Wu Chen, Bo Wu, Renzhong Guo, Xiaoming Li, Chisheng Wang, Weixi Wang
              <br>
              Photogrammetric Engineering & Remote Sensing, 2020
              <br>
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/ETmbpTQidZVDpsfKVwdpqzAB-ntaPzFp8WRePHon9bboBQ?e=rYFnBl"> Link </a> 
              <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;"> In this paper, we describe the trajectory drift‚Äìcompensated strategy that we designed to eliminate the influence of time drift between sensors, remove the inconsistency between the sequences from various sensors, and thereby generate a coarse-to-fine procedure for robust camera-tracking based on two-dimensional‚Äì3D observations from stereo sensors. 
              </p>
            </td>
          </tr>
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/polelike_RS2019.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.mdpi.com/2072-4292/11/24/2920/pdf">
                <papertitle>Pole-Like Street Furniture Segmentation and Classification in Mobile LiDAR Data by Integrating Multiple Shape-Descriptor Constraints </papertitle>
              </a>
              <br>
             You Li, Weixi Wang, Xiaoming Li, Linfu Xie, Yankun Wang, Renzhong Guo, Wenqun Xiu, <strong>Shengjun Tang*</strong>
              <br>
              Remote Sensing, 2019 (SCI, Top)
              <br>
              <a href="https://www.mdpi.com/2072-4292/11/24/2920/pdf"> Link </a> 
              <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;"> We present a complete paradigm for pole-like street furniture segmentation and classification using mobile LiDAR (light detection and ranging) point cloud. 
              </p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/ComBA_ISPRS2016.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/ESkBuqdEjHpMid6GeXsTuzYB70vNeurvdLG7P_-4AjcQeQ?e=gai1V1">
                <papertitle>Combined adjustment of multi-resolution satellite imagery for improved geo-positioning accuracy</papertitle>
              </a>
              <br>
              <strong>Shengjun Tang</strong>, Bo Wu, Qing Zhu
              <br>
              ISPRS Journal of Photogrammetry and Remote Sensing, 2016  (SCI, Top)
              <br>
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/ESkBuqdEjHpMid6GeXsTuzYB70vNeurvdLG7P_-4AjcQeQ?e=gai1V1"> Link </a> 
              <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;"> This paper presents a combined adjustment approach to integrate multi-source multi-resolution satellite imagery for improved geo-positioning accuracy without the use of ground control points (GCPs). 
              </p>
            </td>
          </tr>
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/Com_ISPRS2015.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="http://www.lsgi.polyu.edu.hk/staff/bo.wu/publications/Wu%20et%20al.%202015_Geometric%20Integration%20of%20HRSI%20and%20LiDAR%20Data.pdf">
                <papertitle>Geometric integration of high-resolution satellite imagery and airborne LiDAR data for improved geopositioning accuracy in metropolitan areas</papertitle>
              </a>
              <br>
              Bo Wu, <strong>Shengjun Tang</strong>, Qing Zhu, Kwan-yuen Tong, Han Hu, Guoyuan Li
              <br>
              ISPRS Journal of Photogrammetry and Remote Sensing, 2015  (SCI, Top)
              <br>
              <a href="http://www.lsgi.polyu.edu.hk/staff/bo.wu/publications/Wu%20et%20al.%202015_Geometric%20Integration%20of%20HRSI%20and%20LiDAR%20Data.pdf"> Link </a> 
              <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;">  Considering HRSI and LiDAR datasets taken from metropolitan areas as a case study, this paper presents a novel approach to the geometric integration of HRSI and LiDAR data to reduce their inconsistencies and improve their geopositioning accuracy. 
              </p>
            </td>
          </tr>
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/para2014.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/EdkBd-LtirBLuFLdy87tv7UBAsyvVgggfexuGrIkwKHr9g?e=7QM6lw	">
                <papertitle>‰∏âÁª¥ GIS ‰∏≠ÁöÑÂèÇÊï∞ÂåñÂª∫Ê®°ÊñπÊ≥ï(parametric modeling method in three-dimensional GIS) </papertitle>
              </a>
              <br>
              <strong>Ê±§Âú£Âêõ(Shengjun Tang)</strong>, Âº†Âè∂Âª∑(Yeting Zhang), ËÆ∏‰ºüÂπ≥(Weiping Xu), Ë∞¢ÊΩá(Xiao Xie), Êú±Â∫Ü(Qing Zhu), Èü©ÂÖÉÂà©(Yuanli Han), Âê¥Âº∫(Qiang Wu)
              <br>
              Ê≠¶Ê±âÂ§ßÂ≠¶Â≠¶Êä•Ôºà‰ø°ÊÅØÁßëÂ≠¶ÁâàÔºâ(Geomatics and Information Science of Wuhan University), 2014
              <br>
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/EdkBd-LtirBLuFLdy87tv7UBAsyvVgggfexuGrIkwKHr9g?e=7QM6lw"> Link </a> 
              <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;"> ‰∏∫ÂÆûÁé∞Â§ßËßÑÊ®°Âú∞ÂΩ¢ÊôØËßÇÂíåÁ≤æÁªÜÂ∑•Á®ãËÆæÊñΩÊ®°ÂûãÂú®‰∏âÁª¥GIS‰∏≠ÁöÑÊó†ÁºùÈõÜÊàêÁÆ°ÁêÜ,Âπ∂ÊîØÊåÅÂ∑•Á®ãËÆæÊñΩÂÖ®ÁîüÂëΩÂë®ÊúüÁöÑÂÖ±‰∫´Â∫îÁî®,ÊèêÂá∫‰∫Ü‰∏ÄÁßçÂèØÊ†πÊçÆËÆæËÆ°ÂèÇÊï∞Ëá™Âä®Âª∫Á´ãÂ§çÊùÇËÆæÊñΩ‰∏âÁª¥Ê®°ÂûãÂπ∂‰∫§‰∫íÂºèÁºñËæë‰øÆÊîπÁöÑÊñπÊ≥ï,Êâ©Â±ï‰∫Ü‰∏âÁª¥GISÊï∞ÊçÆÊ®°Âûã,ÂÆûÁé∞‰∫Ü‰∏âÁª¥Âá†‰ΩïÊ®°Âûã‰∏éÂÖ∂ÂèÇÊï∞‰ø°ÊÅØÁöÑÊúâÊú∫ÈõÜÊàê‰∏éÂêåÊ≠•Êõ¥Êñ∞,Âπ∂‰ª•Ê°•Ê¢ÅÊ®°ÂûãÁöÑÊûÑÂª∫‰∏∫‰æãÈ™åËØÅ‰∫ÜËØ•ÊñπÊ≥ïÁöÑÂèØË°åÊÄßÂíåÊúâÊïàÊÄß„ÄÇ(To realize the seamless integrated management of large-scale terrain landscape and detailed engineering facility models in 3D GIS, and to support the shared application of the whole lifecycle of engineering facilities, a method is proposed. This method can automatically build complex facility 3D models based on design parameters and allows interactive editing and modification. This method extends the 3D GIS data model, achieves the organic integration and synchronous update of the 3D geometric model and its parameter information. The feasibility and effectiveness of this method have been verified using the construction of a bridge model as an example.)
              </p>
            </td>
          </tr>
          <tr>
        </tbody></table>
	</section>
	<section id="researchprojects">
	 <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Research projects</heading>
              <p>
                <ul>
		    <li style="text-align:justify; text-justify:inter-ideograph;">‰∏≠Âçé‰∫∫Ê∞ëÂÖ±ÂíåÂõΩÁßëÂ≠¶ÊäÄÊúØÈÉ®ÔºåÂõΩÂÆ∂ÈáçÁÇπÁ†îÂèëËÆ°ÂàíÈ°πÁõÆÂ≠êËØæÈ¢òÔºåË∂ÖÂ§ßÂüéÂ∏ÇÁªøËâ≤‰ΩéÁ¢≥ÂèëÂ±ïÁõëÊµã‰∏éËØäÊñ≠‰ºòÂåñÂ∫îÁî®Á§∫ËåÉ, 2022YFB3903700Ôºå 2022-12 Ëá≥ 2026-11, RMB$248,000,0, PI.</li>
                    <li style="text-align:justify; text-justify:inter-ideograph;">Âπø‰∏úÁúÅÈù¢‰∏äÂü∫ÈáëÔºåËÅîÂêàËßÜËßâSLAM‰∏éÊ∑±Â∫¶Á•ûÁªèÁΩëÁªúÁöÑÂÆ§ÂÜÖÂú∫ÊôØËØ≠‰πâÂàÜÁ±ª‰∏éËá™Âä®ÂåñÂª∫Ê®°ÊäÄÊúØÔºå2021A1515012574Ôºå2021/01/01-2023/12/31ÔºåRMB$100,000ÔºåPI.</li>
                    <li style="text-align:justify; text-justify:inter-ideograph;">Ê∑±Âú≥Â∏ÇÂü∫Á°ÄÁ†îÁ©∂Èù¢‰∏äÈ°πÁõÆÔºåÈù¢ÂêëÊú∫Âô®‰∫∫Ëá™‰∏ªÂØºËà™ÁöÑÁ±ªËÑëËßÜËßâÂú∫ÊôØÁêÜËß£‰∏éÂÆûÊó∂‰ΩçÁΩÆËÆ°ÁÆóÊñπÊ≥ïÔºåJCYJ20210324093012033Ôºå2021/10/28-2024/10/27, RMB$600,000ÔºåPI.</li>
                    <li style="text-align:justify; text-justify:inter-ideograph;">ÂõΩÂÆ∂Ëá™ÁÑ∂ÁßëÂ≠¶Âü∫ÈáëÂßîÈùíÂπ¥Âü∫ÈáëÔºåÂ§öRGB-D‰º†ÊÑüÂô®ËÅîÂêàÁöÑÂú®Á∫øÂÆ§ÂÜÖÈ´òÁ≤æÂ∫¶‰∏âÁª¥ÊµãÂõæÊñπÊ≥ïÔºå41801392Ôºå2019/01/01-2021/12/31, RMB$265,000ÔºåPI.</li>
		    <li style="text-align:justify; text-justify:inter-ideograph;">‰∏≠ÂõΩÂçöÂ£´ÂêéÁßëÂ≠¶Âü∫ÈáëÔºåÂ§öÂÖÉÁâπÂæÅÊ∑∑Âêà‰ºòÂåñÁöÑRGB-DÂÆ§ÂÜÖÈ´òÁ≤æÂ∫¶‰∏âÁª¥ÊµãÂõæÊñπÊ≥ïÔºå2018M633133Ôºå2018/05/01-2019/09/08,RMB$500,00ÔºåPI.</li>	
                    <li style="text-align:justify; text-justify:inter-ideograph;">Ê∑±Âú≥Â∏ÇÁßëÂàõÂßîËá™Áî±Êé¢Á¥¢È°πÁõÆÔºåÂü∫‰∫é‰æøÊê∫ÂºèÊ∑±Â∫¶‰º†ÊÑüÂô®ÁöÑÂüéÂ∏ÇÂ∞ÅÈó≠/ÂçäÂ∞ÅÈó≠Á©∫Èó¥Âø´ÈÄü‰∏âÁª¥ÊµãÂõæÊäÄÊúØÁ†îÁ©∂ÔºåJCYJ20180305125131482Ôºå2019/01/01-2021/12/31, RMB$300,000ÔºåPI.</li>
                    <li style="text-align:justify; text-justify:inter-ideograph;">Ëá™ÁÑ∂ËµÑÊ∫êÈÉ®ÂüéÂ∏ÇËá™ÁÑ∂ËµÑÊ∫êÁõëÊµã‰∏é‰ªøÁúüÈáçÁÇπÂÆûÈ™åÂÆ§ÂºÄÊîæÂü∫Èáë, Âü∫‰∫éÂÆ§ÂÜÖÈ´òÁ≤æÂ∫¶‰∏âÁª¥ÊµãÂõæÁöÑBIMÂÖ≥ÈîÆÈÉ®‰ª∂Ëá™Âä®ÂåñÈáçÂª∫ÊñπÊ≥ïÔºåKF-2019-04-010, 2020/01/17-2021/12/30ÔºåRMB$200,000ÔºåPI.</li>
                     <li style="text-align:justify; text-justify:inter-ideograph;">Ê≠¶Ê±âÂ§ßÂ≠¶ÊµãÁªòÈÅ•ÊÑü‰ø°ÊÅØÂ∑•Á®ãÂõΩÂÆ∂ÈáçÁÇπÂÆûÈ™åÂÆ§ÂºÄÊîæÂü∫ÈáëÔºåÈõÜÊàêËßÜËßâ‰∏éÂá†‰ΩïÁâπÂæÅÁöÑRGB-D SLAMÊñπÊ≥ïÔºå17E04Ôºå2018/01/01-2019/12/31, RMB$500,00ÔºåPI.</li>
                </ul>
              </p>
            </td>
          </tr>
        </tbody></table>
	</section>
	<section id="awards">
	 <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Awards</heading>
              <p>
                <ul>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">The Best Workshop Presentation Award at the ISPRS Geospatial Week 2023 International Conference for 'Tree-GPT: Modular Large Language Model Expert System for Forest Remote Sensing Image Understanding and Interactive Analysis', 2023.</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"> The Second Place of the IPIN 2022 Competition ONLINE Track 2 -Camera in the TWELFTH INTERNATIONAL CONFERENCE ONINDOOR POSITIONING AND INDOORNAVIGATION held in BEIJING, CHINA, on 5-7 SEPTEMBER 2022.</li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">Ê∑±Âú≥Â∏ÇÈ´òÂ±ÇÊ¨°‰∫∫ÊâçCÁ±ª(C-Class High-Level Talents in Shenzhen City), 2019.</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">Âπø‰∏úÁúÅÈ´òÊ†°ÁßëÊäÄÊàêÊûúËΩ¨ÂåñÂ§ßËµõÁîµÂ≠ê‰ø°ÊÅØÁªÑ‰∫åÁ≠âÂ•ñ(Second Prize in the Electronic Information Category of the Guangdong Provincial University Science and Technology Achievement Transformation Competition)<strong>(Rank 2/5)</strong>, 2020.</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">Ê∑±Âú≥Â∏ÇÂçóÂ±±Âå∫"2020Âàõ‰∏ö‰πãÊòüÂ§ßËµõ"‰∫íËÅîÁΩë‰∏éÁßªÂä®‰∫íËÅîÁΩëÂàùÂàõÁªÑ‰∏ÄÁ≠âÂ•ñ(First Prize in the Internet and Mobile Internet Startup Category of the "2020 Entrepreneurship Star Competition" in Nanshan District, Shenzhen City)<strong>(Rank 1/3)</strong>, 2020.</li>
                </ul>
              </p>
            </td>
          </tr>
        </tbody></table>
	</section>
	<section id="academicservices">
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Academic Service</heading>
              <p> Committee Member:
              </p>
              <p>
                <ul>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">Guest Editor for Remote Sensing, <a href="https://www.mdpi.com/journal/remotesensing/special_issues/RMKV4988H7">Special issue on ‚ÄúRemote Sensing for 2D/3D Mapping‚Äù</a>, From 2023</li>
	            <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">Guest Editor for IEEE JSTAR, <a href="https://www.grss-ieee.org/wp-content/uploads/2023/07/cfp_High-fidelity-Urban-3D-Modeling-and-Scene-Simulation.pdf">Special issue on ‚ÄúHigh-fidelity Urban 3D Modeling and Scene Simulation‚Äù</a>, From 2023</li>
	            <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">Senior Editor for The Photogrammetry Record, From 2023</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">ÂõΩÈôÖÊï∞Â≠óÂú∞ÁêÉÂçè‰ºö‰∏≠ÂõΩËôöÊãüÂú∞ÁêÜÁéØÂ¢ÉÂßîÂëò‰ºöÂßîÂëò(Member of the China Virtual Geographic Environment Committee of the International Society for Digital Earth), From 2022</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">‰∏≠ÂõΩÂõæÂ≠¶Â≠¶‰ºöBIM‰∏ìÂßî‰ºöÂßîÂëò(Member of the BIM Special Committee of the Chinese Society for Graphics), From 2022</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">‰∏≠ÂõΩÊµãÁªòÂ≠¶‰ºö‰ΩçÁΩÆÊúçÂä°Â∑•Âßî‰ºöÂßîÂëò(Member of the Location Service Working Committee of the Chinese Society for Surveying and Mapping), From 2022</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">Expert of ISO Technical Committee 59, From 2022</li>
                </ul>
              </p>
              <p> Journal Reviewer:
              </p>
              <p>
                <ul>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"> ISPRS Journal of Photogrammetry and Remote Sensing</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"> The Photogrammetry Record</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"> IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"> Automation In Construction</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"> International Journal of Applied Earth Observation and Geoinformation</li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"> IEEE Transactions on Industrial Informatics</li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"> IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023)</li>
                </ul>
              </p>
            </td>
          </tr>
        </tbody></table>
	</section>
<section id="activites">
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
        <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Academic Activites</heading>
            </td>
        </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/lidarcof.png" alt="PontTuset" width="160" style="border-style: none">
	      <meta name="referrer" content="no-referrer" />
            </td>
            <td width="75%" valign="middle">
                <Conference information> Ê±§Âú£Âêõ‰Ωú‰∏∫ÂÆ§ÂÜÖ‰∏âÁª¥Âú∫ÊôØÁêÜËß£‰∏éÁªìÊûÑÂåñÈáçÂª∫‰∏ìÈ¢ò‰∏ªÂ∏≠ÂèÇÂä†‚ÄúÁ¨¨‰∏ÉÂ±äÂÖ®ÂõΩÊøÄÂÖâÈõ∑ËææÂ§ß‰ºö‚ÄùÔºåÂπ∂ÂÅöÂÆ§ÂÜÖÈ´òÁ≤æÂ∫¶ËßÜËßâ‰ΩçÂßø‰º∞ËÆ°‰∏éARÂØºËà™ÁöÑÊä•Âëä,2023Âπ¥10Êúà20-22Êó•ÔºàTang Shengjun served as the special session chair for indoor 3D scene understanding and structured reconstruction at the "7th National LiDAR Conference". He delivered a report on high-precision indoor visual pose estimation and AR navigation from October 20 to 22, 2023.Ôºâ</Conference information>
              <br>
              Participants: Shengjun Tang
	      <br>
              Presentations : <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/EfUd90HK-6RNsk0WuVqX5MYBBzZjT4GitG2HIWySinMvKA?e=rP1Ss0">ÂÆ§ÂÜÖÈ´òÁ≤æÂ∫¶ËßÜËßâ‰ΩçÂßø‰º∞ËÆ°‰∏éARÂØºËà™ÔºàHigh-precision indoor visual pose estimation and AR navigationÔºâ  </a> 
              <p></p>
            </td>
          </tr>
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/GSW2023.png" alt="PontTuset" width="160" style="border-style: none">
	      <meta name="referrer" content="no-referrer" />
            </td>
            <td width="75%" valign="middle">
                <Conference information> ISPRS Geospatial Week 2023, is held in Cairo, Egypt from September 2-7, 2023.</Conference information>
              <br>
              Participants: Shengjun Tang, Weixi Wang, Xiaoming Li, Linfu Xie, Siqi Du, Hongsheng Huang
	      <br>
              Presentations : <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/Ebo7ZRcMkTtNtDzdI1SbgXkB6xMfxKutIzxLtOMLP-vVEQ?e=1JeYlc">  Benchmark of synthetic indoor scenes </a> |  <a href="https://szueducn-my.sharepoint.com/:p:/g/personal/shengjuntang_szu_edu_cn/EfS0IvsLPwNGktjfu6QkA5IBTvmD1oqtTDqXojbK5_zu-g?e=ZwFbWw"> TreeGPT </a> 
              <p></p>
            </td>
          </tr>
	
		</tbody></table>
	</section>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
        

					
        </tbody></table>
        <p align="center">
          <font size="2">
            Template from <a href="https://shiyujiao.github.io/">shiyu jiao's website</a>.
	 </font>
        </p>
      </td>
    </tr>
  </table>
</body>
</html>
